{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch practice - rec2.ipynb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/McKenzyPG/edu_cm_dl/blob/master/pyTorch_practice_rec2_ipynb.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fIbRqEJq8wnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3687d86b-5366-4667-c889-025d2d7b4ab0"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PVIMhEuY8xvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1adcc48b-527a-4798-bf9e-25085e7d7ea6"
      },
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nmXYFznj89to",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "ccf83d37-9252-4750-c2ad-df04fff989bb"
      },
      "cell_type": "code",
      "source": [
        "# Create uninitialized tensor\n",
        "x = torch.FloatTensor(2,3)\n",
        "print(x)\n",
        "# Initialize to zeros\n",
        "x.zero_()\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.00000e-36 *\n",
            "       [[ 1.0585,  0.0000,  0.0000],\n",
            "        [ 0.0000,     nan,  0.0000]])\n",
            "tensor([[ 0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_V4itcNP8__X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "73d824ef-62b1-428b-a5ec-b0ab1c52ea43"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "# Create from numpy array (seed for repeatability)\n",
        "np.random.seed(123)\n",
        "np_array = np.random.random((2,3))\n",
        "print(torch.FloatTensor(np_array))\n",
        "print(torch.from_numpy(np_array))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6965,  0.2861,  0.2269],\n",
            "        [ 0.5513,  0.7195,  0.4231]])\n",
            "tensor([[ 0.6965,  0.2861,  0.2269],\n",
            "        [ 0.5513,  0.7195,  0.4231]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ybnqVmlY9CL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4b09f5a-535a-43e9-bb14-3fdafbe9b1b4"
      },
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V9Fc8OlY9D7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c79b1049-fcf7-406a-958f-5fa7b7695abe"
      },
      "cell_type": "code",
      "source": [
        "# Create random tensor (seed for repeatability)\n",
        "torch.manual_seed(123)\n",
        "x=torch.randn(2,3)\n",
        "print(x)\n",
        "# export to numpy array\n",
        "x_np = x.numpy()\n",
        "print(x_np)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1115,  0.1204, -0.3696],\n",
            "        [-0.2404, -1.1969,  0.2093]])\n",
            "[[-0.11146712  0.12036294 -0.3696345 ]\n",
            " [-0.24041797 -1.1969243   0.20926936]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BGmORO3i9F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "ba96af18-61a4-40bc-f606-d089a9ff8657"
      },
      "cell_type": "code",
      "source": [
        "# special tensors (see documentation)\n",
        "print(torch.eye(3))\n",
        "print(torch.ones(2,3))\n",
        "print(torch.zeros(2,3))\n",
        "print(torch.arange(0,3))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.,  0.,  0.],\n",
            "        [ 0.,  1.,  0.],\n",
            "        [ 0.,  0.,  1.]])\n",
            "tensor([[ 1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.]])\n",
            "tensor([[ 0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.]])\n",
            "tensor([ 0.,  1.,  2.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "72zBwKl-9HeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fe72467c-f8e8-4ce1-9308-635b82552dd4"
      },
      "cell_type": "code",
      "source": [
        "x=torch.FloatTensor(3,4)\n",
        "print(x.size())\n",
        "print(x.type())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.FloatTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r6NIdLN-9JOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "368191d4-75f2-4e70-8c67-241e318c2c06"
      },
      "cell_type": "code",
      "source": [
        "x = torch.arange(0,5).float()\n",
        "print(torch.sum(x))\n",
        "print(torch.sum(torch.exp(x)))\n",
        "print(torch.mean(x))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.)\n",
            "tensor(85.7910)\n",
            "tensor(2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ot9ffKeX9KiS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "43b9d838-9b97-4358-fc4e-d1d7d61200f6"
      },
      "cell_type": "code",
      "source": [
        "x = torch.rand(3,2)\n",
        "print(x)\n",
        "print(x[1,:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0756,  0.1966],\n",
            "        [ 0.3164,  0.4017],\n",
            "        [ 0.1186,  0.8274]])\n",
            "tensor([ 0.3164,  0.4017])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PLbB5jIE9MFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "962a9055-5166-4bf4-9719-5e88769da841"
      },
      "cell_type": "code",
      "source": [
        "# create a tensor\n",
        "x = torch.rand(3,2)\n",
        "print(x)\n",
        "a = x.cpu()\n",
        "# copy to GPU\n",
        "y = a.cuda()\n",
        "print(y)\n",
        "# copy back to CPU\n",
        "z = y.cpu()\n",
        "print(z)\n",
        "# get CPU tensor as numpy array\n",
        "print(z.numpy())\n",
        "# cannot get GPU tensor as numpy array directly\n",
        "try:\n",
        "  print(y.numpy())\n",
        "except TypeError as e:\n",
        "  print(e)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3821,  0.6605],\n",
            "        [ 0.8536,  0.5932],\n",
            "        [ 0.6367,  0.9826]])\n",
            "tensor([[ 0.3821,  0.6605],\n",
            "        [ 0.8536,  0.5932],\n",
            "        [ 0.6367,  0.9826]], device='cuda:0')\n",
            "tensor([[ 0.3821,  0.6605],\n",
            "        [ 0.8536,  0.5932],\n",
            "        [ 0.6367,  0.9826]])\n",
            "[[0.38208443 0.66049385]\n",
            " [0.8535718  0.593153  ]\n",
            " [0.63672537 0.98262936]]\n",
            "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "liE4QSrv9OLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d29bcd4f-d955-43b6-be01-8f4822223e41"
      },
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q2EQXSfG9VN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82d49a6f-7a4c-4ca6-8972-dbea7bddf898"
      },
      "cell_type": "code",
      "source": [
        "x = torch.rand(3,5)  # CPU tensor\n",
        "y = torch.rand(5,4).cuda()  # GPU tensor\n",
        "try:\n",
        "  torch.mm(x,y)  # Operation between CPU and GPU fails\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'mat2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rI7_zvHV9XCX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Put tensor on CUDA if available\n",
        "x = torch.rand(3,2)\n",
        "if torch.cuda.is_available():\n",
        "  x = x.cuda()\n",
        "\n",
        "# Do some calculations\n",
        "y = x ** 2 \n",
        "\n",
        "# Copy to CPU if on GPU\n",
        "if y.is_cuda:\n",
        "  y = y.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-eqVb-r9Zc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4e8a35c3-ca5c-487a-bef2-6d6ea3711001"
      },
      "cell_type": "code",
      "source": [
        "x1 = torch.rand(3,2)\n",
        "x2 = x1.new(1,2)  # create cpu tensor\n",
        "print(x2)\n",
        "x1 = torch.rand(3,2).cuda()\n",
        "x2 = x1.new(1,2)  # create cuda tensor\n",
        "print(x2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 8.3387e-33,  4.5869e-41]])\n",
            "tensor([[ 0.0090,  0.3351]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "avhfriYs9bTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "63261a27-9a49-494d-edea-cd70dff62978"
      },
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "# Create random data\n",
        "x = torch.rand(1000,64)\n",
        "y = torch.rand(64,32)\n",
        "number = 10000  # number of iterations\n",
        "\n",
        "def square():\n",
        "  z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
        "\n",
        "# Time CPU\n",
        "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
        "# Time GPU\n",
        "x, y = x.cuda(), y.cuda()\n",
        "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU: 787.3146570000245ms\n",
            "GPU: 348.83319700020365ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Rn4BuAq9de8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4be7c5c8-7f66-4b2f-c6a6-e59c3b884801"
      },
      "cell_type": "code",
      "source": [
        "# Create differentiable tensor\n",
        "x = torch.tensor(torch.arange(0,4).float(),requires_grad=True)\n",
        "z = x ** 2\n",
        "b = torch.zeros(4,requires_grad=True)\n",
        "y = 5*z+x+b\n",
        "# Calculate gradient (dy/dx=10x+1,dy/db=1)\n",
        "y.sum().backward()\n",
        "# Print values\n",
        "print(y)\n",
        "print(x.grad)\n",
        "print(b.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  0.,   6.,  22.,  48.])\n",
            "tensor([  1.,  11.,  21.,  31.])\n",
            "tensor([ 1.,  1.,  1.,  1.])\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzoosHpY9fu8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7cffda54-de90-4e24-aca5-1d184143230e"
      },
      "cell_type": "code",
      "source": [
        "# Create a variable\n",
        "x=torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
        "# Differentiate\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Differentiate again (accumulates gradient)\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Zero gradient before differentiating\n",
        "x.grad.data.zero_()\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.,  2.,  4.,  6.])\n",
            "tensor([  0.,   4.,   8.,  12.])\n",
            "tensor([ 0.,  2.,  4.,  6.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lfjzw4Mu9h4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "e3bb9e3a-687c-477f-ea30-5d33b5c2be54"
      },
      "cell_type": "code",
      "source": [
        "x=torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
        "x.numpy() # raises an exception"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-001829cd5143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# raises an exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pR2rYGTg9kfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bffcce2-422a-48e8-ea9c-626b9ae9fd64"
      },
      "cell_type": "code",
      "source": [
        "x=torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
        "y=x**2\n",
        "z=y**2\n",
        "z.detach().numpy()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  1., 16., 81.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "U6Kd9CQU9m0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = torch.nn.Linear(4,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hfkJQ6s9pEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fee703e5-5924-4f4b-dd0f-8ce99bc2cdbb"
      },
      "cell_type": "code",
      "source": [
        "x = torch.arange(0,4).float()\n",
        "y = net.forward(x)\n",
        "y = net(x) # Alternatively\n",
        "print(y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.3535,  0.0932])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xBMajyHA9q13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3502e761-cbce-4e25-bca8-3b112dc1a64e"
      },
      "cell_type": "code",
      "source": [
        "x = torch.arange(0,32).float()\n",
        "net = torch.nn.Linear(32,10)\n",
        "y = net(x)\n",
        "print(y)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ -1.2053, -11.6210,  -7.5367,   9.1668, -14.7067, -13.1726,\n",
            "         11.5341,   3.7762,  23.3194,  -4.3059])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4VukNet89s_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a simple sequential network (`nn.Module` object) from layers (other `nn.Module` objects).\n",
        "# Here a MLP with 2 layers and sigmoid activation.\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(32,128),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Linear(128,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnQ6Y1nl9u3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a more customizable network module (equivalent here)\n",
        "class MyNetwork(torch.nn.Module):\n",
        "    # you can use the layer sizes as initialization arguments if you want to\n",
        "    def __init__(self,input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
        "        self.layer2 = torch.nn.Sigmoid()\n",
        "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
        "\n",
        "    def forward(self, input_val):\n",
        "        h = input_val\n",
        "        h = self.layer1(h)\n",
        "        h = self.layer2(h)\n",
        "        h = self.layer3(h)\n",
        "        return h\n",
        "\n",
        "net = MyNetwork(32,128,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIoy2AQq9xj4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "ff3d86e3-89ad-48bf-c219-d542096e773a"
      },
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "    print(param)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0066, -0.0284, -0.0221,  ..., -0.0455, -0.0637, -0.1313],\n",
            "        [ 0.1603, -0.0496, -0.0025,  ..., -0.1422,  0.1650, -0.0092],\n",
            "        [ 0.1334, -0.0589, -0.1393,  ..., -0.0148, -0.0889,  0.1124],\n",
            "        ...,\n",
            "        [-0.0417,  0.0338, -0.1262,  ...,  0.0331, -0.0907,  0.0445],\n",
            "        [-0.1049,  0.0290, -0.0528,  ...,  0.0237,  0.0159,  0.1424],\n",
            "        [ 0.1321, -0.1763,  0.0188,  ..., -0.0507,  0.1083, -0.1162]])\n",
            "Parameter containing:\n",
            "tensor([-0.0565, -0.0122,  0.0131,  0.0323, -0.1086,  0.1145,  0.1153,\n",
            "         0.1578, -0.1741, -0.0879, -0.1259,  0.1038, -0.1328,  0.1236,\n",
            "         0.0834,  0.1593,  0.1020, -0.1573, -0.0994, -0.0793,  0.0433,\n",
            "        -0.0659, -0.0446, -0.1097, -0.0849, -0.1697, -0.1460,  0.1269,\n",
            "         0.0598,  0.0937,  0.1518, -0.1610,  0.0869, -0.1608, -0.1033,\n",
            "         0.0641, -0.0692, -0.1414,  0.1670, -0.0994,  0.0192, -0.0146,\n",
            "        -0.0799, -0.0333,  0.1528, -0.0130,  0.1032,  0.1355, -0.1415,\n",
            "        -0.1337,  0.0166,  0.1482,  0.1602, -0.0959, -0.0823,  0.1096,\n",
            "         0.0286, -0.1467,  0.0226, -0.1740,  0.1762,  0.1307,  0.0437,\n",
            "         0.0863, -0.1170,  0.0367, -0.1560,  0.0085, -0.0489, -0.1354,\n",
            "         0.1188, -0.1400, -0.1756, -0.1462,  0.0477,  0.0354,  0.1381,\n",
            "        -0.1254,  0.0803, -0.1199, -0.1253,  0.1687,  0.1177, -0.0132,\n",
            "        -0.1615, -0.1667,  0.1310,  0.1598, -0.0513,  0.0045,  0.0911,\n",
            "         0.1045,  0.1245, -0.0221,  0.1442, -0.1270, -0.1057,  0.1460,\n",
            "        -0.0585, -0.0817, -0.0719, -0.0554,  0.0075, -0.0454, -0.1012,\n",
            "         0.0224,  0.1431, -0.1407, -0.0969, -0.1543, -0.0744,  0.0782,\n",
            "        -0.0643, -0.1219, -0.0853,  0.1137, -0.0282,  0.0525,  0.1555,\n",
            "        -0.0164,  0.1385, -0.0724, -0.0203,  0.0768,  0.0048, -0.1763,\n",
            "        -0.0453,  0.1102])\n",
            "Parameter containing:\n",
            "tensor(1.00000e-02 *\n",
            "       [[-3.4930, -4.2943, -5.6617,  ..., -2.6975,  6.5595, -3.9360],\n",
            "        [ 0.3937,  5.3945,  2.0649,  ..., -3.4609,  3.6246, -5.2363],\n",
            "        [-4.2918,  0.2476,  3.7390,  ..., -8.2207,  6.6277, -3.7184],\n",
            "        ...,\n",
            "        [-7.2390, -6.8202, -0.0866,  ...,  2.1890,  3.5134,  7.8017],\n",
            "        [-4.1059, -0.9815,  6.2737,  ...,  1.0904,  2.0460, -6.7180],\n",
            "        [-7.9923, -1.7371,  5.9162,  ...,  3.3254,  0.2410, -7.6028]])\n",
            "Parameter containing:\n",
            "tensor(1.00000e-02 *\n",
            "       [ 7.3244,  1.9835,  2.5361,  5.9860,  2.3242, -5.5696,  0.2733,\n",
            "        -6.0854,  5.2887,  1.4629])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DalBC1Nc9zOm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyNetworkWithParams(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size, output_size):\n",
        "        super(MyNetworkWithParams,self).__init__()\n",
        "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
        "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
        "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
        "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
        "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
        "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
        "        return output\n",
        "\n",
        "net = MyNetworkWithParams(32,128,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CC0YsqvY91dQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = MyNetwork(32,128,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLl_uHOr92wt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e953ad5f-33c4-4a71-df2c-d47230af6320"
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
        "y = torch.tensor([0,3,9])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "output = net(x)\n",
        "loss = criterion(output,y)\n",
        "print(loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3711)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gj3Omq2A937E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a60a354e-923c-4106-857d-6d98402627f1"
      },
      "cell_type": "code",
      "source": [
        "# equivalent\n",
        "criterion2 = nn.NLLLoss()\n",
        "sf = nn.LogSoftmax()\n",
        "output = net(x)\n",
        "loss = criterion(sf(output),y)\n",
        "loss"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3711)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "34PJ7ofE95kl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "cab1056d-8bd4-4f89-95e2-49988969085f"
      },
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "# Check that the parameters now have gradients\n",
        "for param in net.parameters():\n",
        "    print(param.grad)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8298e-03,  2.8301e-03,  2.8303e-03,  ...,  2.8372e-03,\n",
            "          2.8374e-03,  2.8377e-03],\n",
            "        [-3.5337e-03, -3.5337e-03, -3.5337e-03,  ..., -3.5337e-03,\n",
            "         -3.5337e-03, -3.5337e-03],\n",
            "        [-1.6828e-05, -3.1524e-04, -6.1366e-04,  ..., -8.6709e-03,\n",
            "         -8.9693e-03, -9.2677e-03],\n",
            "        ...,\n",
            "        [-5.9205e-03, -5.9166e-03, -5.9127e-03,  ..., -5.8069e-03,\n",
            "         -5.8030e-03, -5.7991e-03],\n",
            "        [-6.7343e-03, -5.0284e-03, -3.3226e-03,  ...,  4.2735e-02,\n",
            "          4.4441e-02,  4.6147e-02],\n",
            "        [ 6.9405e-03,  6.9399e-03,  6.9393e-03,  ...,  6.9234e-03,\n",
            "          6.9228e-03,  6.9222e-03]])\n",
            "tensor(1.00000e-02 *\n",
            "       [ 0.0401, -0.9375, -0.2678,  0.1842, -0.0110,  0.4472, -0.4023,\n",
            "        -1.3054, -0.7353,  0.1489,  0.6351,  0.8315,  0.0682, -0.1357,\n",
            "         0.1471, -0.4267, -0.2310, -0.0537,  0.0924,  0.1252, -0.3606,\n",
            "        -0.6016,  0.0696, -0.4505,  0.1406, -0.0732,  0.3375,  0.4361,\n",
            "         0.2823,  0.2637, -0.3212, -0.3062, -0.0706, -0.1103,  0.6982,\n",
            "        -0.0414,  0.4560, -0.4270, -0.2036, -0.9297,  1.1505,  0.1351,\n",
            "         0.5954,  0.0288, -0.0395, -0.7140, -0.5595, -0.2408,  0.0849,\n",
            "         0.1324,  0.2740,  0.7622,  0.3828, -0.8518, -0.2465,  1.2437,\n",
            "         0.7098,  0.4351, -0.1146,  0.1040,  0.0083,  0.5863,  0.4779,\n",
            "         0.3351, -0.3850, -0.4243,  1.3066,  0.8548,  0.7990, -1.2487,\n",
            "        -0.0032,  0.5235,  0.1578,  0.2994,  0.6741, -0.1017,  1.3268,\n",
            "        -0.5972,  1.2177, -0.4097, -0.1286,  0.4280, -0.2312,  1.1316,\n",
            "        -0.1934, -0.5725,  0.7991, -0.1576, -1.1228, -0.0540, -0.7815,\n",
            "        -0.3066,  0.0680,  1.4284, -0.4617,  1.2219, -0.1828,  0.0328,\n",
            "         0.4291,  0.1296,  0.8306, -0.2283,  0.4836,  0.2849, -0.8948,\n",
            "         0.9049, -0.0826,  0.9118,  0.4094,  0.8072, -0.2944,  0.9916,\n",
            "         0.6703,  0.3650, -0.5785, -0.5061,  0.0739, -0.9119,  0.2312,\n",
            "         0.9313, -0.6356,  0.9070,  0.0495, -0.2614, -0.1583, -0.8123,\n",
            "        -0.3064, -0.0147])\n",
            "tensor([[ 0.0492,  0.0443, -0.0476,  ..., -0.2204, -0.1872,  0.0537],\n",
            "        [ 0.0419,  0.0381,  0.0654,  ...,  0.0790,  0.0756,  0.0454],\n",
            "        [ 0.0207,  0.0187,  0.0361,  ...,  0.0476,  0.0450,  0.0226],\n",
            "        ...,\n",
            "        [ 0.0233,  0.0209,  0.0475,  ...,  0.0698,  0.0651,  0.0257],\n",
            "        [ 0.0270,  0.0242,  0.0554,  ...,  0.0816,  0.0762,  0.0298],\n",
            "        [-0.0904, -0.0613, -0.1636,  ..., -0.1135, -0.1306, -0.1213]])\n",
            "tensor([-0.1628,  0.1276,  0.0718, -0.2771,  0.1077,  0.0628,  0.1129,\n",
            "         0.0974,  0.1138, -0.2542])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jq3-D86h99p2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1421
        },
        "outputId": "b1f9bdab-0160-4976-bba1-97f6d83cd949"
      },
      "cell_type": "code",
      "source": [
        "# if I forward prop and backward prop again, gradients accumulate :\n",
        "output = net(x)\n",
        "loss = criterion(output,y)\n",
        "loss.backward()\n",
        "for param in net.parameters():\n",
        "    print(param.grad)\n",
        "\n",
        "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
        "net.zero_grad()\n",
        "output = net(x)\n",
        "loss = criterion(output,y)\n",
        "loss.backward()\n",
        "for param in net.parameters():\n",
        "    print(param.grad)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0057,  0.0057,  0.0057,  ...,  0.0057,  0.0057,  0.0057],\n",
            "        [-0.0071, -0.0071, -0.0071,  ..., -0.0071, -0.0071, -0.0071],\n",
            "        [-0.0000, -0.0006, -0.0012,  ..., -0.0173, -0.0179, -0.0185],\n",
            "        ...,\n",
            "        [-0.0118, -0.0118, -0.0118,  ..., -0.0116, -0.0116, -0.0116],\n",
            "        [-0.0135, -0.0101, -0.0066,  ...,  0.0855,  0.0889,  0.0923],\n",
            "        [ 0.0139,  0.0139,  0.0139,  ...,  0.0138,  0.0138,  0.0138]])\n",
            "tensor(1.00000e-02 *\n",
            "       [ 0.0802, -1.8751, -0.5357,  0.3685, -0.0219,  0.8943, -0.8046,\n",
            "        -2.6108, -1.4707,  0.2978,  1.2703,  1.6629,  0.1363, -0.2715,\n",
            "         0.2941, -0.8534, -0.4620, -0.1073,  0.1849,  0.2503, -0.7212,\n",
            "        -1.2031,  0.1391, -0.9011,  0.2812, -0.1465,  0.6749,  0.8722,\n",
            "         0.5647,  0.5274, -0.6425, -0.6123, -0.1411, -0.2206,  1.3963,\n",
            "        -0.0829,  0.9119, -0.8540, -0.4071, -1.8594,  2.3011,  0.2701,\n",
            "         1.1908,  0.0576, -0.0790, -1.4280, -1.1190, -0.4816,  0.1697,\n",
            "         0.2649,  0.5480,  1.5243,  0.7657, -1.7035, -0.4930,  2.4874,\n",
            "         1.4196,  0.8702, -0.2291,  0.2079,  0.0165,  1.1727,  0.9559,\n",
            "         0.6703, -0.7700, -0.8487,  2.6133,  1.7097,  1.5981, -2.4974,\n",
            "        -0.0064,  1.0471,  0.3156,  0.5988,  1.3482, -0.2033,  2.6536,\n",
            "        -1.1944,  2.4355, -0.8194, -0.2571,  0.8560, -0.4624,  2.2631,\n",
            "        -0.3868, -1.1451,  1.5982, -0.3152, -2.2456, -0.1079, -1.5630,\n",
            "        -0.6132,  0.1360,  2.8567, -0.9235,  2.4438, -0.3657,  0.0656,\n",
            "         0.8582,  0.2593,  1.6611, -0.4566,  0.9672,  0.5698, -1.7896,\n",
            "         1.8098, -0.1651,  1.8237,  0.8188,  1.6144, -0.5887,  1.9832,\n",
            "         1.3405,  0.7300, -1.1571, -1.0123,  0.1479, -1.8238,  0.4625,\n",
            "         1.8625, -1.2712,  1.8140,  0.0990, -0.5228, -0.3165, -1.6246,\n",
            "        -0.6128, -0.0294])\n",
            "tensor([[ 0.0983,  0.0887, -0.0952,  ..., -0.4409, -0.3743,  0.1074],\n",
            "        [ 0.0838,  0.0763,  0.1308,  ...,  0.1581,  0.1512,  0.0907],\n",
            "        [ 0.0413,  0.0373,  0.0721,  ...,  0.0951,  0.0900,  0.0451],\n",
            "        ...,\n",
            "        [ 0.0467,  0.0418,  0.0950,  ...,  0.1395,  0.1302,  0.0513],\n",
            "        [ 0.0540,  0.0483,  0.1108,  ...,  0.1633,  0.1524,  0.0595],\n",
            "        [-0.1807, -0.1226, -0.3273,  ..., -0.2270, -0.2612, -0.2425]])\n",
            "tensor([-0.3256,  0.2553,  0.1437, -0.5543,  0.2155,  0.1255,  0.2258,\n",
            "         0.1949,  0.2275, -0.5083])\n",
            "tensor([[ 2.8298e-03,  2.8301e-03,  2.8303e-03,  ...,  2.8372e-03,\n",
            "          2.8374e-03,  2.8377e-03],\n",
            "        [-3.5337e-03, -3.5337e-03, -3.5337e-03,  ..., -3.5337e-03,\n",
            "         -3.5337e-03, -3.5337e-03],\n",
            "        [-1.6828e-05, -3.1524e-04, -6.1366e-04,  ..., -8.6709e-03,\n",
            "         -8.9693e-03, -9.2677e-03],\n",
            "        ...,\n",
            "        [-5.9205e-03, -5.9166e-03, -5.9127e-03,  ..., -5.8069e-03,\n",
            "         -5.8030e-03, -5.7991e-03],\n",
            "        [-6.7343e-03, -5.0284e-03, -3.3226e-03,  ...,  4.2735e-02,\n",
            "          4.4441e-02,  4.6147e-02],\n",
            "        [ 6.9405e-03,  6.9399e-03,  6.9393e-03,  ...,  6.9234e-03,\n",
            "          6.9228e-03,  6.9222e-03]])\n",
            "tensor(1.00000e-02 *\n",
            "       [ 0.0401, -0.9375, -0.2678,  0.1842, -0.0110,  0.4472, -0.4023,\n",
            "        -1.3054, -0.7353,  0.1489,  0.6351,  0.8315,  0.0682, -0.1357,\n",
            "         0.1471, -0.4267, -0.2310, -0.0537,  0.0924,  0.1252, -0.3606,\n",
            "        -0.6016,  0.0696, -0.4505,  0.1406, -0.0732,  0.3375,  0.4361,\n",
            "         0.2823,  0.2637, -0.3212, -0.3062, -0.0706, -0.1103,  0.6982,\n",
            "        -0.0414,  0.4560, -0.4270, -0.2036, -0.9297,  1.1505,  0.1351,\n",
            "         0.5954,  0.0288, -0.0395, -0.7140, -0.5595, -0.2408,  0.0849,\n",
            "         0.1324,  0.2740,  0.7622,  0.3828, -0.8518, -0.2465,  1.2437,\n",
            "         0.7098,  0.4351, -0.1146,  0.1040,  0.0083,  0.5863,  0.4779,\n",
            "         0.3351, -0.3850, -0.4243,  1.3066,  0.8548,  0.7990, -1.2487,\n",
            "        -0.0032,  0.5235,  0.1578,  0.2994,  0.6741, -0.1017,  1.3268,\n",
            "        -0.5972,  1.2177, -0.4097, -0.1286,  0.4280, -0.2312,  1.1316,\n",
            "        -0.1934, -0.5725,  0.7991, -0.1576, -1.1228, -0.0540, -0.7815,\n",
            "        -0.3066,  0.0680,  1.4284, -0.4617,  1.2219, -0.1828,  0.0328,\n",
            "         0.4291,  0.1296,  0.8306, -0.2283,  0.4836,  0.2849, -0.8948,\n",
            "         0.9049, -0.0826,  0.9118,  0.4094,  0.8072, -0.2944,  0.9916,\n",
            "         0.6703,  0.3650, -0.5785, -0.5061,  0.0739, -0.9119,  0.2312,\n",
            "         0.9313, -0.6356,  0.9070,  0.0495, -0.2614, -0.1583, -0.8123,\n",
            "        -0.3064, -0.0147])\n",
            "tensor([[ 0.0492,  0.0443, -0.0476,  ..., -0.2204, -0.1872,  0.0537],\n",
            "        [ 0.0419,  0.0381,  0.0654,  ...,  0.0790,  0.0756,  0.0454],\n",
            "        [ 0.0207,  0.0187,  0.0361,  ...,  0.0476,  0.0450,  0.0226],\n",
            "        ...,\n",
            "        [ 0.0233,  0.0209,  0.0475,  ...,  0.0698,  0.0651,  0.0257],\n",
            "        [ 0.0270,  0.0242,  0.0554,  ...,  0.0816,  0.0762,  0.0298],\n",
            "        [-0.0904, -0.0613, -0.1636,  ..., -0.1135, -0.1306, -0.1213]])\n",
            "tensor([-0.1628,  0.1276,  0.0718, -0.2771,  0.1077,  0.0628,  0.1129,\n",
            "         0.0974,  0.1138, -0.2542])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xgBjUh2N-AH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1529
        },
        "outputId": "618c1d23-1e73-4067-8885-29be3e428df7"
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Parameters before gradient descent :\")\n",
        "for param in net.parameters():\n",
        "    print(param)\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Parameters after gradient descent :\")\n",
        "for param in net.parameters():\n",
        "    print(param)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters before gradient descent :\n",
            "Parameter containing:\n",
            "tensor([[-0.0257, -0.0565, -0.1145,  ...,  0.0207, -0.0604, -0.1375],\n",
            "        [-0.1278, -0.1447,  0.0621,  ..., -0.1384, -0.0902,  0.0665],\n",
            "        [ 0.1418,  0.1006, -0.1525,  ...,  0.0008, -0.0297, -0.1108],\n",
            "        ...,\n",
            "        [-0.1192, -0.0836,  0.1603,  ...,  0.1452, -0.0735,  0.1366],\n",
            "        [-0.0811, -0.0257,  0.0106,  ..., -0.0972, -0.0423,  0.0740],\n",
            "        [ 0.0780,  0.0622,  0.0414,  ...,  0.1607, -0.1137, -0.0695]])\n",
            "Parameter containing:\n",
            "tensor([ 0.0403,  0.0866, -0.0381, -0.0676, -0.1215,  0.1454,  0.1548,\n",
            "         0.0473,  0.0093, -0.0777,  0.1307,  0.0738,  0.0428, -0.0367,\n",
            "         0.1464,  0.1611,  0.0209,  0.0431, -0.1234, -0.0167,  0.1560,\n",
            "        -0.0413, -0.1622,  0.1103, -0.0161, -0.0406, -0.0504,  0.0779,\n",
            "         0.0155,  0.0946,  0.0614, -0.1548,  0.0500, -0.1348,  0.0805,\n",
            "        -0.0520,  0.0702, -0.1589,  0.0230,  0.1121, -0.0054, -0.0841,\n",
            "        -0.0954,  0.1534,  0.1668,  0.0355, -0.1320,  0.0758,  0.0252,\n",
            "        -0.1717, -0.1480,  0.1100, -0.0301, -0.1244,  0.0388,  0.1223,\n",
            "         0.1289, -0.0404,  0.0010,  0.1011, -0.0074,  0.1627,  0.1615,\n",
            "        -0.1063, -0.0807,  0.0380, -0.0965, -0.0350, -0.0039, -0.0983,\n",
            "        -0.0435, -0.0097, -0.0312, -0.1722,  0.0109,  0.0997, -0.0460,\n",
            "         0.0446,  0.0779,  0.0752,  0.0099, -0.1635, -0.1513, -0.1321,\n",
            "        -0.0327, -0.1355,  0.0731,  0.0599, -0.1695, -0.1178,  0.0159,\n",
            "         0.0258, -0.1056, -0.1238, -0.0915,  0.0889,  0.1685,  0.0304,\n",
            "         0.0478,  0.1049,  0.0916,  0.1560,  0.1470,  0.0880, -0.0451,\n",
            "         0.0965, -0.1220, -0.1361, -0.1619,  0.0362,  0.0510,  0.0733,\n",
            "         0.0527,  0.1688,  0.1271, -0.0429,  0.1150,  0.0908, -0.1154,\n",
            "         0.0167, -0.1524,  0.1416,  0.1616, -0.1383,  0.0019,  0.0224,\n",
            "        -0.1226, -0.0443])\n",
            "Parameter containing:\n",
            "tensor(1.00000e-02 *\n",
            "       [[-7.3723, -8.0999,  0.1882,  ..., -5.5933, -3.2844,  0.8366],\n",
            "        [ 0.1800,  2.1475, -8.1548,  ..., -8.1774, -5.9715,  1.0793],\n",
            "        [-6.6866,  3.4125,  3.1412,  ..., -5.2781,  3.2705,  6.5886],\n",
            "        ...,\n",
            "        [-0.7326, -4.8669, -8.4164,  ...,  7.0197,  5.0700,  5.5238],\n",
            "        [ 2.9823, -6.7712, -4.5125,  ..., -3.9608,  4.7763,  1.8399],\n",
            "        [-5.6542,  4.2782, -0.0499,  ...,  3.8918,  8.7112, -8.5176]])\n",
            "Parameter containing:\n",
            "tensor(1.00000e-02 *\n",
            "       [ 2.0528,  5.5047,  0.9465, -4.5720, -5.8382, -2.1054, -4.6179,\n",
            "         3.8285, -2.6362, -4.8494])\n",
            "Parameters after gradient descent :\n",
            "Parameter containing:\n",
            "tensor([[-0.0257, -0.0566, -0.1145,  ...,  0.0206, -0.0605, -0.1375],\n",
            "        [-0.1278, -0.1447,  0.0621,  ..., -0.1383, -0.0902,  0.0665],\n",
            "        [ 0.1418,  0.1006, -0.1525,  ...,  0.0009, -0.0297, -0.1107],\n",
            "        ...,\n",
            "        [-0.1192, -0.0836,  0.1603,  ...,  0.1452, -0.0734,  0.1367],\n",
            "        [-0.0810, -0.0256,  0.0106,  ..., -0.0976, -0.0428,  0.0735],\n",
            "        [ 0.0780,  0.0621,  0.0413,  ...,  0.1607, -0.1138, -0.0695]])\n",
            "Parameter containing:\n",
            "tensor([ 0.0403,  0.0867, -0.0380, -0.0676, -0.1215,  0.1454,  0.1549,\n",
            "         0.0474,  0.0094, -0.0777,  0.1306,  0.0737,  0.0428, -0.0367,\n",
            "         0.1463,  0.1611,  0.0210,  0.0431, -0.1234, -0.0167,  0.1561,\n",
            "        -0.0413, -0.1622,  0.1104, -0.0161, -0.0406, -0.0505,  0.0778,\n",
            "         0.0155,  0.0946,  0.0615, -0.1548,  0.0500, -0.1348,  0.0805,\n",
            "        -0.0520,  0.0702, -0.1589,  0.0230,  0.1122, -0.0055, -0.0842,\n",
            "        -0.0954,  0.1534,  0.1668,  0.0356, -0.1319,  0.0758,  0.0252,\n",
            "        -0.1717, -0.1480,  0.1099, -0.0301, -0.1243,  0.0388,  0.1222,\n",
            "         0.1288, -0.0405,  0.0010,  0.1011, -0.0074,  0.1627,  0.1615,\n",
            "        -0.1063, -0.0807,  0.0381, -0.0966, -0.0351, -0.0040, -0.0981,\n",
            "        -0.0435, -0.0098, -0.0312, -0.1723,  0.0108,  0.0997, -0.0462,\n",
            "         0.0447,  0.0778,  0.0752,  0.0099, -0.1635, -0.1513, -0.1322,\n",
            "        -0.0327, -0.1355,  0.0730,  0.0599, -0.1694, -0.1178,  0.0160,\n",
            "         0.0259, -0.1056, -0.1240, -0.0914,  0.0888,  0.1685,  0.0304,\n",
            "         0.0478,  0.1049,  0.0916,  0.1560,  0.1469,  0.0880, -0.0450,\n",
            "         0.0965, -0.1220, -0.1362, -0.1620,  0.0361,  0.0511,  0.0732,\n",
            "         0.0526,  0.1688,  0.1272, -0.0428,  0.1150,  0.0909, -0.1154,\n",
            "         0.0166, -0.1523,  0.1416,  0.1616, -0.1383,  0.0019,  0.0224,\n",
            "        -0.1226, -0.0443])\n",
            "Parameter containing:\n",
            "tensor(1.00000e-02 *\n",
            "       [[-7.4215, -8.1443,  0.2358,  ..., -5.3728, -3.0973,  0.7829],\n",
            "        [ 0.1382,  2.1094, -8.2202,  ..., -8.2564, -6.0471,  1.0340],\n",
            "        [-6.7073,  3.3938,  3.1051,  ..., -5.3257,  3.2255,  6.5661],\n",
            "        ...,\n",
            "        [-0.7559, -4.8878, -8.4639,  ...,  6.9500,  5.0049,  5.4981],\n",
            "        [ 2.9553, -6.7953, -4.5679,  ..., -4.0425,  4.7001,  1.8101],\n",
            "        [-5.5639,  4.3395,  0.1138,  ...,  4.0053,  8.8418, -8.3963]])\n",
            "Parameter containing:\n",
            "tensor(1.00000e-02 *\n",
            "       [ 2.2156,  5.3770,  0.8747, -4.2949, -5.9460, -2.1681, -4.7308,\n",
            "         3.7310, -2.7499, -4.5952])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4PNcHClZ-DVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23183
        },
        "outputId": "658fa39b-b099-41b8-d5b5-6dca34ea512f"
      },
      "cell_type": "code",
      "source": [
        "# In a training loop, we should perform many GD iterations.\n",
        "n_iter = 1000\n",
        "for i in range(n_iter):\n",
        "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
        "    output = net(x)\n",
        "    loss = criterion(output,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2448)\n",
            "tensor(2.1325)\n",
            "tensor(2.0309)\n",
            "tensor(1.9385)\n",
            "tensor(1.8542)\n",
            "tensor(1.7770)\n",
            "tensor(1.7062)\n",
            "tensor(1.6413)\n",
            "tensor(1.5820)\n",
            "tensor(1.5278)\n",
            "tensor(1.4779)\n",
            "tensor(1.4319)\n",
            "tensor(1.3893)\n",
            "tensor(1.3498)\n",
            "tensor(1.3130)\n",
            "tensor(1.2786)\n",
            "tensor(1.2464)\n",
            "tensor(1.2162)\n",
            "tensor(1.1878)\n",
            "tensor(1.1611)\n",
            "tensor(1.1358)\n",
            "tensor(1.1119)\n",
            "tensor(1.0891)\n",
            "tensor(1.0675)\n",
            "tensor(1.0467)\n",
            "tensor(1.0268)\n",
            "tensor(1.0077)\n",
            "tensor(0.9894)\n",
            "tensor(0.9718)\n",
            "tensor(0.9550)\n",
            "tensor(0.9389)\n",
            "tensor(0.9236)\n",
            "tensor(0.9090)\n",
            "tensor(0.8952)\n",
            "tensor(0.8821)\n",
            "tensor(0.8696)\n",
            "tensor(0.8577)\n",
            "tensor(0.8464)\n",
            "tensor(0.8355)\n",
            "tensor(0.8251)\n",
            "tensor(0.8151)\n",
            "tensor(0.8055)\n",
            "tensor(0.7962)\n",
            "tensor(0.7873)\n",
            "tensor(0.7787)\n",
            "tensor(0.7703)\n",
            "tensor(0.7622)\n",
            "tensor(0.7544)\n",
            "tensor(0.7469)\n",
            "tensor(0.7395)\n",
            "tensor(0.7324)\n",
            "tensor(0.7255)\n",
            "tensor(0.7187)\n",
            "tensor(0.7122)\n",
            "tensor(0.7059)\n",
            "tensor(0.6997)\n",
            "tensor(0.6936)\n",
            "tensor(0.6878)\n",
            "tensor(0.6820)\n",
            "tensor(0.6765)\n",
            "tensor(0.6710)\n",
            "tensor(0.6657)\n",
            "tensor(0.6605)\n",
            "tensor(0.6554)\n",
            "tensor(0.6505)\n",
            "tensor(0.6456)\n",
            "tensor(0.6409)\n",
            "tensor(0.6362)\n",
            "tensor(0.6317)\n",
            "tensor(0.6272)\n",
            "tensor(0.6229)\n",
            "tensor(0.6186)\n",
            "tensor(0.6144)\n",
            "tensor(0.6103)\n",
            "tensor(0.6063)\n",
            "tensor(0.6023)\n",
            "tensor(0.5985)\n",
            "tensor(0.5947)\n",
            "tensor(0.5909)\n",
            "tensor(0.5872)\n",
            "tensor(0.5836)\n",
            "tensor(0.5801)\n",
            "tensor(0.5766)\n",
            "tensor(0.5732)\n",
            "tensor(0.5698)\n",
            "tensor(0.5665)\n",
            "tensor(0.5632)\n",
            "tensor(0.5600)\n",
            "tensor(0.5569)\n",
            "tensor(0.5538)\n",
            "tensor(0.5507)\n",
            "tensor(0.5477)\n",
            "tensor(0.5447)\n",
            "tensor(0.5418)\n",
            "tensor(0.5389)\n",
            "tensor(0.5360)\n",
            "tensor(0.5332)\n",
            "tensor(0.5305)\n",
            "tensor(0.5277)\n",
            "tensor(0.5250)\n",
            "tensor(0.5224)\n",
            "tensor(0.5198)\n",
            "tensor(0.5172)\n",
            "tensor(0.5146)\n",
            "tensor(0.5121)\n",
            "tensor(0.5096)\n",
            "tensor(0.5072)\n",
            "tensor(0.5047)\n",
            "tensor(0.5023)\n",
            "tensor(0.5000)\n",
            "tensor(0.4976)\n",
            "tensor(0.4953)\n",
            "tensor(0.4930)\n",
            "tensor(0.4907)\n",
            "tensor(0.4885)\n",
            "tensor(0.4863)\n",
            "tensor(0.4841)\n",
            "tensor(0.4819)\n",
            "tensor(0.4798)\n",
            "tensor(0.4777)\n",
            "tensor(0.4756)\n",
            "tensor(0.4735)\n",
            "tensor(0.4715)\n",
            "tensor(0.4694)\n",
            "tensor(0.4674)\n",
            "tensor(0.4654)\n",
            "tensor(0.4634)\n",
            "tensor(0.4615)\n",
            "tensor(0.4596)\n",
            "tensor(0.4576)\n",
            "tensor(0.4557)\n",
            "tensor(0.4539)\n",
            "tensor(0.4520)\n",
            "tensor(0.4502)\n",
            "tensor(0.4483)\n",
            "tensor(0.4465)\n",
            "tensor(0.4447)\n",
            "tensor(0.4429)\n",
            "tensor(0.4412)\n",
            "tensor(0.4394)\n",
            "tensor(0.4377)\n",
            "tensor(0.4360)\n",
            "tensor(0.4343)\n",
            "tensor(0.4326)\n",
            "tensor(0.4309)\n",
            "tensor(0.4292)\n",
            "tensor(0.4276)\n",
            "tensor(0.4260)\n",
            "tensor(0.4243)\n",
            "tensor(0.4227)\n",
            "tensor(0.4211)\n",
            "tensor(0.4195)\n",
            "tensor(0.4180)\n",
            "tensor(0.4164)\n",
            "tensor(0.4148)\n",
            "tensor(0.4133)\n",
            "tensor(0.4118)\n",
            "tensor(0.4103)\n",
            "tensor(0.4088)\n",
            "tensor(0.4073)\n",
            "tensor(0.4058)\n",
            "tensor(0.4043)\n",
            "tensor(0.4028)\n",
            "tensor(0.4014)\n",
            "tensor(0.4000)\n",
            "tensor(0.3985)\n",
            "tensor(0.3971)\n",
            "tensor(0.3957)\n",
            "tensor(0.3943)\n",
            "tensor(0.3929)\n",
            "tensor(0.3915)\n",
            "tensor(0.3901)\n",
            "tensor(0.3888)\n",
            "tensor(0.3874)\n",
            "tensor(0.3860)\n",
            "tensor(0.3847)\n",
            "tensor(0.3834)\n",
            "tensor(0.3820)\n",
            "tensor(0.3807)\n",
            "tensor(0.3794)\n",
            "tensor(0.3781)\n",
            "tensor(0.3768)\n",
            "tensor(0.3755)\n",
            "tensor(0.3743)\n",
            "tensor(0.3730)\n",
            "tensor(0.3717)\n",
            "tensor(0.3705)\n",
            "tensor(0.3692)\n",
            "tensor(0.3680)\n",
            "tensor(0.3668)\n",
            "tensor(0.3655)\n",
            "tensor(0.3643)\n",
            "tensor(0.3631)\n",
            "tensor(0.3619)\n",
            "tensor(0.3607)\n",
            "tensor(0.3595)\n",
            "tensor(0.3583)\n",
            "tensor(0.3572)\n",
            "tensor(0.3560)\n",
            "tensor(0.3548)\n",
            "tensor(0.3537)\n",
            "tensor(0.3525)\n",
            "tensor(0.3514)\n",
            "tensor(0.3502)\n",
            "tensor(0.3491)\n",
            "tensor(0.3480)\n",
            "tensor(0.3468)\n",
            "tensor(0.3457)\n",
            "tensor(0.3446)\n",
            "tensor(0.3435)\n",
            "tensor(0.3424)\n",
            "tensor(0.3413)\n",
            "tensor(0.3402)\n",
            "tensor(0.3391)\n",
            "tensor(0.3381)\n",
            "tensor(0.3370)\n",
            "tensor(0.3359)\n",
            "tensor(0.3349)\n",
            "tensor(0.3338)\n",
            "tensor(0.3327)\n",
            "tensor(0.3317)\n",
            "tensor(0.3307)\n",
            "tensor(0.3296)\n",
            "tensor(0.3286)\n",
            "tensor(0.3276)\n",
            "tensor(0.3265)\n",
            "tensor(0.3255)\n",
            "tensor(0.3245)\n",
            "tensor(0.3235)\n",
            "tensor(0.3225)\n",
            "tensor(0.3215)\n",
            "tensor(0.3205)\n",
            "tensor(0.3195)\n",
            "tensor(0.3185)\n",
            "tensor(0.3176)\n",
            "tensor(0.3166)\n",
            "tensor(0.3156)\n",
            "tensor(0.3147)\n",
            "tensor(0.3137)\n",
            "tensor(0.3127)\n",
            "tensor(0.3118)\n",
            "tensor(0.3108)\n",
            "tensor(0.3099)\n",
            "tensor(0.3089)\n",
            "tensor(0.3080)\n",
            "tensor(0.3071)\n",
            "tensor(0.3062)\n",
            "tensor(0.3052)\n",
            "tensor(0.3043)\n",
            "tensor(0.3034)\n",
            "tensor(0.3025)\n",
            "tensor(0.3016)\n",
            "tensor(0.3007)\n",
            "tensor(0.2998)\n",
            "tensor(0.2989)\n",
            "tensor(0.2980)\n",
            "tensor(0.2971)\n",
            "tensor(0.2962)\n",
            "tensor(0.2953)\n",
            "tensor(0.2944)\n",
            "tensor(0.2936)\n",
            "tensor(0.2927)\n",
            "tensor(0.2918)\n",
            "tensor(0.2910)\n",
            "tensor(0.2901)\n",
            "tensor(0.2893)\n",
            "tensor(0.2884)\n",
            "tensor(0.2876)\n",
            "tensor(0.2867)\n",
            "tensor(0.2859)\n",
            "tensor(0.2850)\n",
            "tensor(0.2842)\n",
            "tensor(0.2834)\n",
            "tensor(0.2825)\n",
            "tensor(0.2817)\n",
            "tensor(0.2809)\n",
            "tensor(0.2801)\n",
            "tensor(0.2793)\n",
            "tensor(0.2785)\n",
            "tensor(0.2776)\n",
            "tensor(0.2768)\n",
            "tensor(0.2760)\n",
            "tensor(0.2752)\n",
            "tensor(0.2744)\n",
            "tensor(0.2737)\n",
            "tensor(0.2729)\n",
            "tensor(0.2721)\n",
            "tensor(0.2713)\n",
            "tensor(0.2705)\n",
            "tensor(0.2697)\n",
            "tensor(0.2690)\n",
            "tensor(0.2682)\n",
            "tensor(0.2674)\n",
            "tensor(0.2667)\n",
            "tensor(0.2659)\n",
            "tensor(0.2651)\n",
            "tensor(0.2644)\n",
            "tensor(0.2636)\n",
            "tensor(0.2629)\n",
            "tensor(0.2621)\n",
            "tensor(0.2614)\n",
            "tensor(0.2607)\n",
            "tensor(0.2599)\n",
            "tensor(0.2592)\n",
            "tensor(0.2585)\n",
            "tensor(0.2577)\n",
            "tensor(0.2570)\n",
            "tensor(0.2563)\n",
            "tensor(0.2556)\n",
            "tensor(0.2548)\n",
            "tensor(0.2541)\n",
            "tensor(0.2534)\n",
            "tensor(0.2527)\n",
            "tensor(0.2520)\n",
            "tensor(0.2513)\n",
            "tensor(0.2506)\n",
            "tensor(0.2499)\n",
            "tensor(0.2492)\n",
            "tensor(0.2485)\n",
            "tensor(0.2478)\n",
            "tensor(0.2471)\n",
            "tensor(0.2464)\n",
            "tensor(0.2457)\n",
            "tensor(0.2451)\n",
            "tensor(0.2444)\n",
            "tensor(0.2437)\n",
            "tensor(0.2430)\n",
            "tensor(0.2424)\n",
            "tensor(0.2417)\n",
            "tensor(0.2410)\n",
            "tensor(0.2404)\n",
            "tensor(0.2397)\n",
            "tensor(0.2390)\n",
            "tensor(0.2384)\n",
            "tensor(0.2377)\n",
            "tensor(0.2371)\n",
            "tensor(0.2364)\n",
            "tensor(0.2358)\n",
            "tensor(0.2351)\n",
            "tensor(0.2345)\n",
            "tensor(0.2338)\n",
            "tensor(0.2332)\n",
            "tensor(0.2326)\n",
            "tensor(0.2319)\n",
            "tensor(0.2313)\n",
            "tensor(0.2307)\n",
            "tensor(0.2301)\n",
            "tensor(0.2294)\n",
            "tensor(0.2288)\n",
            "tensor(0.2282)\n",
            "tensor(0.2276)\n",
            "tensor(0.2270)\n",
            "tensor(0.2264)\n",
            "tensor(0.2257)\n",
            "tensor(0.2251)\n",
            "tensor(0.2245)\n",
            "tensor(0.2239)\n",
            "tensor(0.2233)\n",
            "tensor(0.2227)\n",
            "tensor(0.2221)\n",
            "tensor(0.2215)\n",
            "tensor(0.2209)\n",
            "tensor(0.2204)\n",
            "tensor(0.2198)\n",
            "tensor(0.2192)\n",
            "tensor(0.2186)\n",
            "tensor(0.2180)\n",
            "tensor(0.2174)\n",
            "tensor(0.2169)\n",
            "tensor(0.2163)\n",
            "tensor(0.2157)\n",
            "tensor(0.2151)\n",
            "tensor(0.2146)\n",
            "tensor(0.2140)\n",
            "tensor(0.2134)\n",
            "tensor(0.2129)\n",
            "tensor(0.2123)\n",
            "tensor(0.2117)\n",
            "tensor(0.2112)\n",
            "tensor(0.2106)\n",
            "tensor(0.2101)\n",
            "tensor(0.2095)\n",
            "tensor(0.2090)\n",
            "tensor(0.2084)\n",
            "tensor(0.2079)\n",
            "tensor(0.2073)\n",
            "tensor(0.2068)\n",
            "tensor(0.2063)\n",
            "tensor(0.2057)\n",
            "tensor(0.2052)\n",
            "tensor(0.2047)\n",
            "tensor(0.2041)\n",
            "tensor(0.2036)\n",
            "tensor(0.2031)\n",
            "tensor(0.2025)\n",
            "tensor(0.2020)\n",
            "tensor(0.2015)\n",
            "tensor(0.2010)\n",
            "tensor(0.2005)\n",
            "tensor(0.1999)\n",
            "tensor(0.1994)\n",
            "tensor(0.1989)\n",
            "tensor(0.1984)\n",
            "tensor(0.1979)\n",
            "tensor(0.1974)\n",
            "tensor(0.1969)\n",
            "tensor(0.1964)\n",
            "tensor(0.1959)\n",
            "tensor(0.1954)\n",
            "tensor(0.1949)\n",
            "tensor(0.1944)\n",
            "tensor(0.1939)\n",
            "tensor(0.1934)\n",
            "tensor(0.1929)\n",
            "tensor(0.1924)\n",
            "tensor(0.1919)\n",
            "tensor(0.1914)\n",
            "tensor(0.1909)\n",
            "tensor(0.1904)\n",
            "tensor(0.1900)\n",
            "tensor(0.1895)\n",
            "tensor(0.1890)\n",
            "tensor(0.1885)\n",
            "tensor(0.1881)\n",
            "tensor(0.1876)\n",
            "tensor(0.1871)\n",
            "tensor(0.1866)\n",
            "tensor(0.1862)\n",
            "tensor(0.1857)\n",
            "tensor(0.1852)\n",
            "tensor(0.1848)\n",
            "tensor(0.1843)\n",
            "tensor(0.1838)\n",
            "tensor(0.1834)\n",
            "tensor(0.1829)\n",
            "tensor(0.1825)\n",
            "tensor(0.1820)\n",
            "tensor(0.1816)\n",
            "tensor(0.1811)\n",
            "tensor(0.1807)\n",
            "tensor(0.1802)\n",
            "tensor(0.1798)\n",
            "tensor(0.1793)\n",
            "tensor(0.1789)\n",
            "tensor(0.1784)\n",
            "tensor(0.1780)\n",
            "tensor(0.1776)\n",
            "tensor(0.1771)\n",
            "tensor(0.1767)\n",
            "tensor(0.1763)\n",
            "tensor(0.1758)\n",
            "tensor(0.1754)\n",
            "tensor(0.1750)\n",
            "tensor(0.1745)\n",
            "tensor(0.1741)\n",
            "tensor(0.1737)\n",
            "tensor(0.1733)\n",
            "tensor(0.1728)\n",
            "tensor(0.1724)\n",
            "tensor(0.1720)\n",
            "tensor(0.1716)\n",
            "tensor(0.1712)\n",
            "tensor(0.1707)\n",
            "tensor(0.1703)\n",
            "tensor(0.1699)\n",
            "tensor(0.1695)\n",
            "tensor(0.1691)\n",
            "tensor(0.1687)\n",
            "tensor(0.1683)\n",
            "tensor(0.1679)\n",
            "tensor(0.1675)\n",
            "tensor(0.1671)\n",
            "tensor(0.1667)\n",
            "tensor(0.1663)\n",
            "tensor(0.1659)\n",
            "tensor(0.1655)\n",
            "tensor(0.1651)\n",
            "tensor(0.1647)\n",
            "tensor(0.1643)\n",
            "tensor(0.1639)\n",
            "tensor(0.1635)\n",
            "tensor(0.1631)\n",
            "tensor(0.1627)\n",
            "tensor(0.1623)\n",
            "tensor(0.1620)\n",
            "tensor(0.1616)\n",
            "tensor(0.1612)\n",
            "tensor(0.1608)\n",
            "tensor(0.1604)\n",
            "tensor(0.1600)\n",
            "tensor(0.1597)\n",
            "tensor(0.1593)\n",
            "tensor(0.1589)\n",
            "tensor(0.1585)\n",
            "tensor(0.1582)\n",
            "tensor(0.1578)\n",
            "tensor(0.1574)\n",
            "tensor(0.1571)\n",
            "tensor(0.1567)\n",
            "tensor(0.1563)\n",
            "tensor(0.1560)\n",
            "tensor(0.1556)\n",
            "tensor(0.1552)\n",
            "tensor(0.1549)\n",
            "tensor(0.1545)\n",
            "tensor(0.1542)\n",
            "tensor(0.1538)\n",
            "tensor(0.1534)\n",
            "tensor(0.1531)\n",
            "tensor(0.1527)\n",
            "tensor(0.1524)\n",
            "tensor(0.1520)\n",
            "tensor(0.1517)\n",
            "tensor(0.1513)\n",
            "tensor(0.1510)\n",
            "tensor(0.1506)\n",
            "tensor(0.1503)\n",
            "tensor(0.1499)\n",
            "tensor(0.1496)\n",
            "tensor(0.1493)\n",
            "tensor(0.1489)\n",
            "tensor(0.1486)\n",
            "tensor(0.1482)\n",
            "tensor(0.1479)\n",
            "tensor(0.1476)\n",
            "tensor(0.1472)\n",
            "tensor(0.1469)\n",
            "tensor(0.1466)\n",
            "tensor(0.1462)\n",
            "tensor(0.1459)\n",
            "tensor(0.1456)\n",
            "tensor(0.1452)\n",
            "tensor(0.1449)\n",
            "tensor(0.1446)\n",
            "tensor(0.1442)\n",
            "tensor(0.1439)\n",
            "tensor(0.1436)\n",
            "tensor(0.1433)\n",
            "tensor(0.1430)\n",
            "tensor(0.1426)\n",
            "tensor(0.1423)\n",
            "tensor(0.1420)\n",
            "tensor(0.1417)\n",
            "tensor(0.1414)\n",
            "tensor(0.1410)\n",
            "tensor(0.1407)\n",
            "tensor(0.1404)\n",
            "tensor(0.1401)\n",
            "tensor(0.1398)\n",
            "tensor(0.1395)\n",
            "tensor(0.1392)\n",
            "tensor(0.1389)\n",
            "tensor(0.1386)\n",
            "tensor(0.1383)\n",
            "tensor(0.1379)\n",
            "tensor(0.1376)\n",
            "tensor(0.1373)\n",
            "tensor(0.1370)\n",
            "tensor(0.1367)\n",
            "tensor(0.1364)\n",
            "tensor(0.1361)\n",
            "tensor(0.1358)\n",
            "tensor(0.1355)\n",
            "tensor(0.1352)\n",
            "tensor(0.1349)\n",
            "tensor(0.1347)\n",
            "tensor(0.1344)\n",
            "tensor(0.1341)\n",
            "tensor(0.1338)\n",
            "tensor(0.1335)\n",
            "tensor(0.1332)\n",
            "tensor(0.1329)\n",
            "tensor(0.1326)\n",
            "tensor(0.1323)\n",
            "tensor(0.1320)\n",
            "tensor(0.1318)\n",
            "tensor(0.1315)\n",
            "tensor(0.1312)\n",
            "tensor(0.1309)\n",
            "tensor(0.1306)\n",
            "tensor(0.1303)\n",
            "tensor(0.1301)\n",
            "tensor(0.1298)\n",
            "tensor(0.1295)\n",
            "tensor(0.1292)\n",
            "tensor(0.1290)\n",
            "tensor(0.1287)\n",
            "tensor(0.1284)\n",
            "tensor(0.1281)\n",
            "tensor(0.1279)\n",
            "tensor(0.1276)\n",
            "tensor(0.1273)\n",
            "tensor(0.1271)\n",
            "tensor(0.1268)\n",
            "tensor(0.1265)\n",
            "tensor(0.1262)\n",
            "tensor(0.1260)\n",
            "tensor(0.1257)\n",
            "tensor(0.1255)\n",
            "tensor(0.1252)\n",
            "tensor(0.1249)\n",
            "tensor(0.1247)\n",
            "tensor(0.1244)\n",
            "tensor(0.1241)\n",
            "tensor(0.1239)\n",
            "tensor(0.1236)\n",
            "tensor(0.1234)\n",
            "tensor(0.1231)\n",
            "tensor(0.1228)\n",
            "tensor(0.1226)\n",
            "tensor(0.1223)\n",
            "tensor(0.1221)\n",
            "tensor(0.1218)\n",
            "tensor(0.1216)\n",
            "tensor(0.1213)\n",
            "tensor(0.1211)\n",
            "tensor(0.1208)\n",
            "tensor(0.1206)\n",
            "tensor(0.1203)\n",
            "tensor(0.1201)\n",
            "tensor(0.1198)\n",
            "tensor(0.1196)\n",
            "tensor(0.1193)\n",
            "tensor(0.1191)\n",
            "tensor(0.1189)\n",
            "tensor(0.1186)\n",
            "tensor(0.1184)\n",
            "tensor(0.1181)\n",
            "tensor(0.1179)\n",
            "tensor(0.1176)\n",
            "tensor(0.1174)\n",
            "tensor(0.1172)\n",
            "tensor(0.1169)\n",
            "tensor(0.1167)\n",
            "tensor(0.1165)\n",
            "tensor(0.1162)\n",
            "tensor(0.1160)\n",
            "tensor(0.1157)\n",
            "tensor(0.1155)\n",
            "tensor(0.1153)\n",
            "tensor(0.1150)\n",
            "tensor(0.1148)\n",
            "tensor(0.1146)\n",
            "tensor(0.1144)\n",
            "tensor(0.1141)\n",
            "tensor(0.1139)\n",
            "tensor(0.1137)\n",
            "tensor(0.1134)\n",
            "tensor(0.1132)\n",
            "tensor(0.1130)\n",
            "tensor(0.1128)\n",
            "tensor(0.1125)\n",
            "tensor(0.1123)\n",
            "tensor(0.1121)\n",
            "tensor(0.1119)\n",
            "tensor(0.1117)\n",
            "tensor(0.1114)\n",
            "tensor(0.1112)\n",
            "tensor(0.1110)\n",
            "tensor(0.1108)\n",
            "tensor(0.1106)\n",
            "tensor(0.1103)\n",
            "tensor(0.1101)\n",
            "tensor(0.1099)\n",
            "tensor(0.1097)\n",
            "tensor(0.1095)\n",
            "tensor(0.1093)\n",
            "tensor(0.1090)\n",
            "tensor(0.1088)\n",
            "tensor(0.1086)\n",
            "tensor(0.1084)\n",
            "tensor(0.1082)\n",
            "tensor(0.1080)\n",
            "tensor(0.1078)\n",
            "tensor(0.1076)\n",
            "tensor(0.1074)\n",
            "tensor(0.1071)\n",
            "tensor(0.1069)\n",
            "tensor(0.1067)\n",
            "tensor(0.1065)\n",
            "tensor(0.1063)\n",
            "tensor(0.1061)\n",
            "tensor(0.1059)\n",
            "tensor(0.1057)\n",
            "tensor(0.1055)\n",
            "tensor(0.1053)\n",
            "tensor(0.1051)\n",
            "tensor(0.1049)\n",
            "tensor(0.1047)\n",
            "tensor(0.1045)\n",
            "tensor(0.1043)\n",
            "tensor(0.1041)\n",
            "tensor(0.1039)\n",
            "tensor(0.1037)\n",
            "tensor(0.1035)\n",
            "tensor(0.1033)\n",
            "tensor(0.1031)\n",
            "tensor(0.1029)\n",
            "tensor(0.1027)\n",
            "tensor(0.1025)\n",
            "tensor(0.1023)\n",
            "tensor(0.1021)\n",
            "tensor(0.1020)\n",
            "tensor(0.1018)\n",
            "tensor(0.1016)\n",
            "tensor(0.1014)\n",
            "tensor(0.1012)\n",
            "tensor(0.1010)\n",
            "tensor(0.1008)\n",
            "tensor(0.1006)\n",
            "tensor(0.1004)\n",
            "tensor(0.1003)\n",
            "tensor(0.1001)\n",
            "tensor(1.00000e-02 *\n",
            "       9.9878)\n",
            "tensor(1.00000e-02 *\n",
            "       9.9692)\n",
            "tensor(1.00000e-02 *\n",
            "       9.9507)\n",
            "tensor(1.00000e-02 *\n",
            "       9.9322)\n",
            "tensor(1.00000e-02 *\n",
            "       9.9138)\n",
            "tensor(1.00000e-02 *\n",
            "       9.8955)\n",
            "tensor(1.00000e-02 *\n",
            "       9.8772)\n",
            "tensor(1.00000e-02 *\n",
            "       9.8589)\n",
            "tensor(1.00000e-02 *\n",
            "       9.8407)\n",
            "tensor(1.00000e-02 *\n",
            "       9.8226)\n",
            "tensor(1.00000e-02 *\n",
            "       9.8045)\n",
            "tensor(1.00000e-02 *\n",
            "       9.7865)\n",
            "tensor(1.00000e-02 *\n",
            "       9.7685)\n",
            "tensor(1.00000e-02 *\n",
            "       9.7505)\n",
            "tensor(1.00000e-02 *\n",
            "       9.7327)\n",
            "tensor(1.00000e-02 *\n",
            "       9.7148)\n",
            "tensor(1.00000e-02 *\n",
            "       9.6971)\n",
            "tensor(1.00000e-02 *\n",
            "       9.6793)\n",
            "tensor(1.00000e-02 *\n",
            "       9.6617)\n",
            "tensor(1.00000e-02 *\n",
            "       9.6441)\n",
            "tensor(1.00000e-02 *\n",
            "       9.6265)\n",
            "tensor(1.00000e-02 *\n",
            "       9.6090)\n",
            "tensor(1.00000e-02 *\n",
            "       9.5915)\n",
            "tensor(1.00000e-02 *\n",
            "       9.5741)\n",
            "tensor(1.00000e-02 *\n",
            "       9.5567)\n",
            "tensor(1.00000e-02 *\n",
            "       9.5394)\n",
            "tensor(1.00000e-02 *\n",
            "       9.5221)\n",
            "tensor(1.00000e-02 *\n",
            "       9.5049)\n",
            "tensor(1.00000e-02 *\n",
            "       9.4877)\n",
            "tensor(1.00000e-02 *\n",
            "       9.4706)\n",
            "tensor(1.00000e-02 *\n",
            "       9.4536)\n",
            "tensor(1.00000e-02 *\n",
            "       9.4365)\n",
            "tensor(1.00000e-02 *\n",
            "       9.4196)\n",
            "tensor(1.00000e-02 *\n",
            "       9.4026)\n",
            "tensor(1.00000e-02 *\n",
            "       9.3858)\n",
            "tensor(1.00000e-02 *\n",
            "       9.3689)\n",
            "tensor(1.00000e-02 *\n",
            "       9.3522)\n",
            "tensor(1.00000e-02 *\n",
            "       9.3354)\n",
            "tensor(1.00000e-02 *\n",
            "       9.3187)\n",
            "tensor(1.00000e-02 *\n",
            "       9.3021)\n",
            "tensor(1.00000e-02 *\n",
            "       9.2855)\n",
            "tensor(1.00000e-02 *\n",
            "       9.2690)\n",
            "tensor(1.00000e-02 *\n",
            "       9.2525)\n",
            "tensor(1.00000e-02 *\n",
            "       9.2360)\n",
            "tensor(1.00000e-02 *\n",
            "       9.2196)\n",
            "tensor(1.00000e-02 *\n",
            "       9.2033)\n",
            "tensor(1.00000e-02 *\n",
            "       9.1870)\n",
            "tensor(1.00000e-02 *\n",
            "       9.1707)\n",
            "tensor(1.00000e-02 *\n",
            "       9.1545)\n",
            "tensor(1.00000e-02 *\n",
            "       9.1383)\n",
            "tensor(1.00000e-02 *\n",
            "       9.1222)\n",
            "tensor(1.00000e-02 *\n",
            "       9.1061)\n",
            "tensor(1.00000e-02 *\n",
            "       9.0901)\n",
            "tensor(1.00000e-02 *\n",
            "       9.0741)\n",
            "tensor(1.00000e-02 *\n",
            "       9.0581)\n",
            "tensor(1.00000e-02 *\n",
            "       9.0422)\n",
            "tensor(1.00000e-02 *\n",
            "       9.0264)\n",
            "tensor(1.00000e-02 *\n",
            "       9.0106)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9948)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9791)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9634)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9478)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9322)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9166)\n",
            "tensor(1.00000e-02 *\n",
            "       8.9011)\n",
            "tensor(1.00000e-02 *\n",
            "       8.8857)\n",
            "tensor(1.00000e-02 *\n",
            "       8.8703)\n",
            "tensor(1.00000e-02 *\n",
            "       8.8549)\n",
            "tensor(1.00000e-02 *\n",
            "       8.8396)\n",
            "tensor(1.00000e-02 *\n",
            "       8.8243)\n",
            "tensor(1.00000e-02 *\n",
            "       8.8090)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7938)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7787)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7635)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7485)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7334)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7184)\n",
            "tensor(1.00000e-02 *\n",
            "       8.7035)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6886)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6737)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6589)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6441)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6293)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6146)\n",
            "tensor(1.00000e-02 *\n",
            "       8.6000)\n",
            "tensor(1.00000e-02 *\n",
            "       8.5853)\n",
            "tensor(1.00000e-02 *\n",
            "       8.5707)\n",
            "tensor(1.00000e-02 *\n",
            "       8.5562)\n",
            "tensor(1.00000e-02 *\n",
            "       8.5417)\n",
            "tensor(1.00000e-02 *\n",
            "       8.5272)\n",
            "tensor(1.00000e-02 *\n",
            "       8.5128)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4984)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4841)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4698)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4555)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4413)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4271)\n",
            "tensor(1.00000e-02 *\n",
            "       8.4129)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3988)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3847)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3707)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3567)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3427)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3288)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3149)\n",
            "tensor(1.00000e-02 *\n",
            "       8.3010)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2872)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2735)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2597)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2460)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2324)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2187)\n",
            "tensor(1.00000e-02 *\n",
            "       8.2051)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1916)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1781)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1646)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1511)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1377)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1244)\n",
            "tensor(1.00000e-02 *\n",
            "       8.1110)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0977)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0845)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0712)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0580)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0449)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0317)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0186)\n",
            "tensor(1.00000e-02 *\n",
            "       8.0056)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9926)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9796)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9666)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9537)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9408)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9280)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9151)\n",
            "tensor(1.00000e-02 *\n",
            "       7.9024)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8896)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8769)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8642)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8516)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8390)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8264)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8138)\n",
            "tensor(1.00000e-02 *\n",
            "       7.8013)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7888)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7764)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7639)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7516)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7392)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7269)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7146)\n",
            "tensor(1.00000e-02 *\n",
            "       7.7023)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6901)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6779)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6658)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6536)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6415)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6295)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6174)\n",
            "tensor(1.00000e-02 *\n",
            "       7.6054)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5935)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5815)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5696)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5577)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5459)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5340)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5223)\n",
            "tensor(1.00000e-02 *\n",
            "       7.5105)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4988)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4871)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4754)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4637)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4521)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4406)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4290)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4175)\n",
            "tensor(1.00000e-02 *\n",
            "       7.4060)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3945)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3831)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3717)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3603)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3490)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3377)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3264)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3151)\n",
            "tensor(1.00000e-02 *\n",
            "       7.3039)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2927)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2815)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2704)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2592)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2482)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2371)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2261)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2150)\n",
            "tensor(1.00000e-02 *\n",
            "       7.2041)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1931)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1822)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1713)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1604)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1496)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1388)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1280)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1172)\n",
            "tensor(1.00000e-02 *\n",
            "       7.1065)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0958)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0851)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0744)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0638)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0532)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0426)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0321)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0216)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0111)\n",
            "tensor(1.00000e-02 *\n",
            "       7.0006)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9902)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9798)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9694)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9590)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9486)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9383)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9280)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9178)\n",
            "tensor(1.00000e-02 *\n",
            "       6.9075)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8973)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8871)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8770)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8668)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8567)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8466)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8366)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8265)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8165)\n",
            "tensor(1.00000e-02 *\n",
            "       6.8065)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7965)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7866)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7767)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7668)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7569)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7471)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7372)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7274)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7177)\n",
            "tensor(1.00000e-02 *\n",
            "       6.7079)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6982)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6885)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6788)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6691)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6595)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6499)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6403)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6307)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6212)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6117)\n",
            "tensor(1.00000e-02 *\n",
            "       6.6022)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5927)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5833)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5738)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5644)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5550)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5457)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5363)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5270)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5177)\n",
            "tensor(1.00000e-02 *\n",
            "       6.5085)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4992)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4900)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4808)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4716)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4624)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4533)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4442)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4351)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4260)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4169)\n",
            "tensor(1.00000e-02 *\n",
            "       6.4079)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3989)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3899)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3809)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3720)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3631)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3541)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3453)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3364)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3276)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3187)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3099)\n",
            "tensor(1.00000e-02 *\n",
            "       6.3012)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xjRwEraW-Fvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1e8abc05-f856-432d-e4be-108b9b9350a8"
      },
      "cell_type": "code",
      "source": [
        "output = net(x)\n",
        "print(output)\n",
        "print(y)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 7.6518, -2.2572, -2.0078,  0.7327, -1.9824, -2.1911, -1.9617,\n",
            "         -1.5904, -1.6133,  3.7098],\n",
            "        [ 0.3031, -1.3820, -1.6498,  5.7447, -1.5390, -1.6038, -1.5714,\n",
            "         -1.7296, -1.6747,  3.1288],\n",
            "        [ 1.7986, -1.6997, -1.7378,  2.9429, -1.6754, -1.7895, -1.6576,\n",
            "         -1.6598, -1.6758,  5.6404]])\n",
            "tensor([ 0,  3,  9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YRASLGvW-KUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0524c91-d4ff-4e81-e7ef-f899e9d2420f"
      },
      "cell_type": "code",
      "source": [
        "# get dictionary of keys to weights using `state_dict`\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(28*28,256),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Linear(256,10))\n",
        "print(net.state_dict().keys())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "keR-TWZ5-MkY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save a dictionary\n",
        "torch.save(net.state_dict(),'test.t7')\n",
        "# load a dictionary\n",
        "net.load_state_dict(torch.load('test.t7'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHZdeNzd-N5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "221b615a-b38a-4a9f-f798-6e0077eb7fec"
      },
      "cell_type": "code",
      "source": [
        "net = nn.Linear(4,2)\n",
        "x = torch.tensor([1,2,3,4])\n",
        "y = net(x)\n",
        "print(y)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-4d1c8f2c4847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'mat2'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IzV0X4LF-PRF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = x.float()\n",
        "x = torch.tensor([1.,2.,3.,4.])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N07YqAC5-Rq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a5ce9846-597d-4a6b-ab67-48ab53a711d4"
      },
      "cell_type": "code",
      "source": [
        "x = 2* torch.ones(2,2)\n",
        "y = 3* torch.ones(2,2)\n",
        "print(x * y)\n",
        "print(x.matmul(y))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.,  6.],\n",
            "        [ 6.,  6.]])\n",
            "tensor([[ 12.,  12.],\n",
            "        [ 12.,  12.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i6EJM2bm-Ts2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "6fbcca7f-a184-4e41-d9fa-ff61b731d1b9"
      },
      "cell_type": "code",
      "source": [
        "x = torch.ones(4,5).float()\n",
        "y = torch.arange(5).float()\n",
        "print(x+y)\n",
        "y = torch.arange(4).float().view(-1,1)\n",
        "print(x+y)\n",
        "y = torch.arange(4).float()\n",
        "print(x+y) # exception"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
            "        [ 1.,  2.,  3.,  4.,  5.],\n",
            "        [ 1.,  2.,  3.,  4.,  5.],\n",
            "        [ 1.,  2.,  3.,  4.,  5.]])\n",
            "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
            "        [ 2.,  2.,  2.,  2.,  2.],\n",
            "        [ 3.,  3.,  3.,  3.,  3.],\n",
            "        [ 4.,  4.,  4.,  4.,  4.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-21069971df37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DwYqjLqJ-VDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c25bc107-2cd0-4bec-a5cb-9c5a2a7af265"
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(x)\n",
        "print(x.t())\n",
        "print(x.view(3,2))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6]])\n",
            "tensor([[ 1,  4],\n",
            "        [ 2,  5],\n",
            "        [ 3,  6]])\n",
            "tensor([[ 1,  2],\n",
            "        [ 3,  4],\n",
            "        [ 5,  6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5owoEA29-XQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6e8aa37-24f9-4e26-8828-e8b6ce05cf0a"
      },
      "cell_type": "code",
      "source": [
        "def get_layer():\n",
        "    return nn.Sequential(nn.Linear(2048,2048),nn.ReLU())\n",
        "def get_layers(n):\n",
        "    return nn.Sequential(*[get_layer() for i in range(n)])\n",
        "net = nn.Sequential(get_layers(100),\n",
        "                   nn.Linear(2048,120))\n",
        "x = torch.rand(1024,2048)\n",
        "y = torch.zeros(1024).long()\n",
        "net=net.cuda()\n",
        "x=x.cuda()\n",
        "y=y.cuda()\n",
        "crit=nn.CrossEntropyLoss()\n",
        "out = net(x)\n",
        "loss = crit(out,y)\n",
        "loss.backward()\n",
        "print(loss)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.7652, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eyh8BSGo-ZRh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self,n_hidden_layers):\n",
        "        super(MyNet,self).__init__()\n",
        "        self.n_hidden_layers=n_hidden_layers\n",
        "        self.final_layer = nn.Linear(128,10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.hidden = []\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden.append(nn.Linear(128,128))\n",
        "    \n",
        "            \n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            h = self.hidden[i](h)\n",
        "            h = self.act(h)\n",
        "        out = self.final_layer(h)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ySA8K3MA-bz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self,n_hidden_layers):\n",
        "        super(MyNet,self).__init__()\n",
        "        self.n_hidden_layers=n_hidden_layers\n",
        "        self.final_layer = nn.Linear(128,10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.hidden = []\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden.append(nn.Linear(128,128))\n",
        "        self.hidden = nn.ModuleList(self.hidden)\n",
        "            \n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            h = self.hidden[i](h)\n",
        "            h = self.act(h)\n",
        "        out = self.final_layer(h)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJJRV1Fx-dPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8m9mNKrK-gog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_points(n):\n",
        "    # returns (X,Y), where X of shape (n,2) is the numpy array of points and Y is the (n) array of classes\n",
        "    \n",
        "    radius = np.random.uniform(low=0,high=2,size=n).reshape(-1,1) # uniform radius between 0 and 2\n",
        "    angle = np.random.uniform(low=0,high=2*np.pi,size=n).reshape(-1,1) # uniform angle\n",
        "    x1 = radius*np.cos(angle)\n",
        "    x2=radius*np.sin(angle)\n",
        "    y = (radius<1).astype(int).reshape(-1)\n",
        "    x = np.concatenate([x1,x2],axis=1)\n",
        "    return x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvIVCWTa-iE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6e3b294-9464-411c-da22-10b87e0432a8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate the data\n",
        "trainx,trainy = sample_points(10000)\n",
        "valx,valy = sample_points(500)\n",
        "testx,testy = sample_points(500)\n",
        "\n",
        "print(trainx.shape,trainy.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 2) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "slNrZO1I-jnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_single_hidden_MLP(n_hidden_neurons):\n",
        "    return nn.Sequential(nn.Linear(2,n_hidden_neurons),nn.ReLU(),nn.Linear(n_hidden_neurons,2))\n",
        "model1 = generate_single_hidden_MLP(6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bje9MZ6N-l-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fb57463-7154-4c91-a0b3-3a4db5bf6246"
      },
      "cell_type": "code",
      "source": [
        "trainx = torch.from_numpy(trainx).float()\n",
        "valx = torch.from_numpy(valx).float()\n",
        "testx = torch.from_numpy(testx).float()\n",
        "trainy = torch.from_numpy(trainy)\n",
        "valy = torch.from_numpy(valy)\n",
        "testy = torch.from_numpy(testy)\n",
        "print(trainx.type(),trainy.type())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.FloatTensor torch.LongTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EhP4lPTO-nwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_routine(net,dataset,n_iters,gpu):\n",
        "    # organize the data\n",
        "    train_data,train_labels,val_data,val_labels = dataset\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(),lr=0.01)\n",
        "    \n",
        "    # use the flag\n",
        "    if gpu:\n",
        "        train_data,train_labels = train_data.cuda(),train_labels.cuda()\n",
        "        val_data,val_labels = val_data.cuda(),val_labels.cuda()\n",
        "        net = net.cuda() # the network parameters also need to be on the gpu !\n",
        "        print(\"Using GPU\")\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "    for i in range(n_iters):\n",
        "        # forward pass\n",
        "        train_output = net(train_data)\n",
        "        train_loss = criterion(train_output,train_labels)\n",
        "        # backward pass and optimization\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Once every 100 iterations, print statistics\n",
        "        if i%100==0:\n",
        "            print(\"At iteration\",i)\n",
        "            # compute the accuracy of the prediction\n",
        "            train_prediction = train_output.cpu().detach().argmax(dim=1)\n",
        "            train_accuracy = (train_prediction.numpy()==train_labels.numpy()).mean() \n",
        "            # Now for the validation set\n",
        "            val_output = net(val_data)\n",
        "            val_loss = criterion(val_output,val_labels)\n",
        "            # compute the accuracy of the prediction\n",
        "            val_prediction = val_output.cpu().detach().argmax(dim=1)\n",
        "            val_accuracy = (val_prediction.numpy()==val_labels.numpy()).mean() \n",
        "            print(\"Training loss :\",train_loss.cpu().detach().numpy())\n",
        "            print(\"Training accuracy :\",train_accuracy)\n",
        "            print(\"Validation loss :\",val_loss.cpu().detach().numpy())\n",
        "            print(\"Validation accuracy :\",val_accuracy)\n",
        "    \n",
        "    net = net.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7HmstNtL-qDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = trainx,trainy,valx,valy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-BtHRxp-r7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9035
        },
        "outputId": "ab1fcb95-1c00-474b-a13d-5e134449eda6"
      },
      "cell_type": "code",
      "source": [
        "gpu = False\n",
        "gpu = gpu and torch.cuda.is_available() # to know if you actually can use the GPU\n",
        "begin = time.time()\n",
        "training_routine(model1,dataset,10000,gpu)\n",
        "end=time.time()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "At iteration 0\n",
            "Training loss : 0.72586685\n",
            "Training accuracy : 0.4965\n",
            "Validation loss : 0.7369952\n",
            "Validation accuracy : 0.472\n",
            "At iteration 100\n",
            "Training loss : 0.6646706\n",
            "Training accuracy : 0.6647\n",
            "Validation loss : 0.66756636\n",
            "Validation accuracy : 0.658\n",
            "At iteration 200\n",
            "Training loss : 0.6423394\n",
            "Training accuracy : 0.7952\n",
            "Validation loss : 0.6434726\n",
            "Validation accuracy : 0.798\n",
            "At iteration 300\n",
            "Training loss : 0.62422746\n",
            "Training accuracy : 0.8025\n",
            "Validation loss : 0.62552905\n",
            "Validation accuracy : 0.794\n",
            "At iteration 400\n",
            "Training loss : 0.6059362\n",
            "Training accuracy : 0.806\n",
            "Validation loss : 0.60770285\n",
            "Validation accuracy : 0.798\n",
            "At iteration 500\n",
            "Training loss : 0.5869266\n",
            "Training accuracy : 0.816\n",
            "Validation loss : 0.58919376\n",
            "Validation accuracy : 0.806\n",
            "At iteration 600\n",
            "Training loss : 0.5671713\n",
            "Training accuracy : 0.8357\n",
            "Validation loss : 0.57004344\n",
            "Validation accuracy : 0.82\n",
            "At iteration 700\n",
            "Training loss : 0.5466578\n",
            "Training accuracy : 0.8529\n",
            "Validation loss : 0.5502337\n",
            "Validation accuracy : 0.842\n",
            "At iteration 800\n",
            "Training loss : 0.5255108\n",
            "Training accuracy : 0.8699\n",
            "Validation loss : 0.529854\n",
            "Validation accuracy : 0.858\n",
            "At iteration 900\n",
            "Training loss : 0.50385773\n",
            "Training accuracy : 0.8859\n",
            "Validation loss : 0.50876826\n",
            "Validation accuracy : 0.87\n",
            "At iteration 1000\n",
            "Training loss : 0.48193535\n",
            "Training accuracy : 0.9002\n",
            "Validation loss : 0.48722452\n",
            "Validation accuracy : 0.892\n",
            "At iteration 1100\n",
            "Training loss : 0.46002227\n",
            "Training accuracy : 0.9131\n",
            "Validation loss : 0.46554834\n",
            "Validation accuracy : 0.898\n",
            "At iteration 1200\n",
            "Training loss : 0.43850625\n",
            "Training accuracy : 0.923\n",
            "Validation loss : 0.44417346\n",
            "Validation accuracy : 0.918\n",
            "At iteration 1300\n",
            "Training loss : 0.41761205\n",
            "Training accuracy : 0.9319\n",
            "Validation loss : 0.42333663\n",
            "Validation accuracy : 0.924\n",
            "At iteration 1400\n",
            "Training loss : 0.3975515\n",
            "Training accuracy : 0.9399\n",
            "Validation loss : 0.4034168\n",
            "Validation accuracy : 0.936\n",
            "At iteration 1500\n",
            "Training loss : 0.37848938\n",
            "Training accuracy : 0.9469\n",
            "Validation loss : 0.3843934\n",
            "Validation accuracy : 0.938\n",
            "At iteration 1600\n",
            "Training loss : 0.36043623\n",
            "Training accuracy : 0.9526\n",
            "Validation loss : 0.36633807\n",
            "Validation accuracy : 0.938\n",
            "At iteration 1700\n",
            "Training loss : 0.34348896\n",
            "Training accuracy : 0.9558\n",
            "Validation loss : 0.34949997\n",
            "Validation accuracy : 0.94\n",
            "At iteration 1800\n",
            "Training loss : 0.32766527\n",
            "Training accuracy : 0.9605\n",
            "Validation loss : 0.33382326\n",
            "Validation accuracy : 0.95\n",
            "At iteration 1900\n",
            "Training loss : 0.3129368\n",
            "Training accuracy : 0.9636\n",
            "Validation loss : 0.31922737\n",
            "Validation accuracy : 0.954\n",
            "At iteration 2000\n",
            "Training loss : 0.29930705\n",
            "Training accuracy : 0.9666\n",
            "Validation loss : 0.3057262\n",
            "Validation accuracy : 0.966\n",
            "At iteration 2100\n",
            "Training loss : 0.28674603\n",
            "Training accuracy : 0.9688\n",
            "Validation loss : 0.2933104\n",
            "Validation accuracy : 0.968\n",
            "At iteration 2200\n",
            "Training loss : 0.27519962\n",
            "Training accuracy : 0.9704\n",
            "Validation loss : 0.28186837\n",
            "Validation accuracy : 0.97\n",
            "At iteration 2300\n",
            "Training loss : 0.26457354\n",
            "Training accuracy : 0.9718\n",
            "Validation loss : 0.27127966\n",
            "Validation accuracy : 0.972\n",
            "At iteration 2400\n",
            "Training loss : 0.254771\n",
            "Training accuracy : 0.9733\n",
            "Validation loss : 0.26152113\n",
            "Validation accuracy : 0.972\n",
            "At iteration 2500\n",
            "Training loss : 0.24572456\n",
            "Training accuracy : 0.9737\n",
            "Validation loss : 0.25253808\n",
            "Validation accuracy : 0.968\n",
            "At iteration 2600\n",
            "Training loss : 0.23735994\n",
            "Training accuracy : 0.9741\n",
            "Validation loss : 0.24424304\n",
            "Validation accuracy : 0.968\n",
            "At iteration 2700\n",
            "Training loss : 0.2296138\n",
            "Training accuracy : 0.9752\n",
            "Validation loss : 0.23655984\n",
            "Validation accuracy : 0.968\n",
            "At iteration 2800\n",
            "Training loss : 0.22243342\n",
            "Training accuracy : 0.9758\n",
            "Validation loss : 0.22942209\n",
            "Validation accuracy : 0.97\n",
            "At iteration 2900\n",
            "Training loss : 0.21576433\n",
            "Training accuracy : 0.9762\n",
            "Validation loss : 0.22282308\n",
            "Validation accuracy : 0.97\n",
            "At iteration 3000\n",
            "Training loss : 0.20956326\n",
            "Training accuracy : 0.9768\n",
            "Validation loss : 0.21670096\n",
            "Validation accuracy : 0.972\n",
            "At iteration 3100\n",
            "Training loss : 0.20377986\n",
            "Training accuracy : 0.9774\n",
            "Validation loss : 0.21100628\n",
            "Validation accuracy : 0.972\n",
            "At iteration 3200\n",
            "Training loss : 0.19837785\n",
            "Training accuracy : 0.9776\n",
            "Validation loss : 0.20568712\n",
            "Validation accuracy : 0.972\n",
            "At iteration 3300\n",
            "Training loss : 0.193318\n",
            "Training accuracy : 0.978\n",
            "Validation loss : 0.20071624\n",
            "Validation accuracy : 0.978\n",
            "At iteration 3400\n",
            "Training loss : 0.18856922\n",
            "Training accuracy : 0.9786\n",
            "Validation loss : 0.1960607\n",
            "Validation accuracy : 0.978\n",
            "At iteration 3500\n",
            "Training loss : 0.1841067\n",
            "Training accuracy : 0.9788\n",
            "Validation loss : 0.1916866\n",
            "Validation accuracy : 0.976\n",
            "At iteration 3600\n",
            "Training loss : 0.17990696\n",
            "Training accuracy : 0.9791\n",
            "Validation loss : 0.18757485\n",
            "Validation accuracy : 0.976\n",
            "At iteration 3700\n",
            "Training loss : 0.17594786\n",
            "Training accuracy : 0.9794\n",
            "Validation loss : 0.18370199\n",
            "Validation accuracy : 0.978\n",
            "At iteration 3800\n",
            "Training loss : 0.17220864\n",
            "Training accuracy : 0.9799\n",
            "Validation loss : 0.18002877\n",
            "Validation accuracy : 0.98\n",
            "At iteration 3900\n",
            "Training loss : 0.16867469\n",
            "Training accuracy : 0.9801\n",
            "Validation loss : 0.17656589\n",
            "Validation accuracy : 0.98\n",
            "At iteration 4000\n",
            "Training loss : 0.16532901\n",
            "Training accuracy : 0.9803\n",
            "Validation loss : 0.17328419\n",
            "Validation accuracy : 0.98\n",
            "At iteration 4100\n",
            "Training loss : 0.1621576\n",
            "Training accuracy : 0.9806\n",
            "Validation loss : 0.17016892\n",
            "Validation accuracy : 0.98\n",
            "At iteration 4200\n",
            "Training loss : 0.15914914\n",
            "Training accuracy : 0.9807\n",
            "Validation loss : 0.16721168\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4300\n",
            "Training loss : 0.15629096\n",
            "Training accuracy : 0.9811\n",
            "Validation loss : 0.16440201\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4400\n",
            "Training loss : 0.15357006\n",
            "Training accuracy : 0.9815\n",
            "Validation loss : 0.16173385\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4500\n",
            "Training loss : 0.15097773\n",
            "Training accuracy : 0.9817\n",
            "Validation loss : 0.15919149\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4600\n",
            "Training loss : 0.1485049\n",
            "Training accuracy : 0.982\n",
            "Validation loss : 0.15676185\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4700\n",
            "Training loss : 0.1461415\n",
            "Training accuracy : 0.9823\n",
            "Validation loss : 0.15444133\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4800\n",
            "Training loss : 0.14388086\n",
            "Training accuracy : 0.9823\n",
            "Validation loss : 0.15221775\n",
            "Validation accuracy : 0.982\n",
            "At iteration 4900\n",
            "Training loss : 0.14171357\n",
            "Training accuracy : 0.9824\n",
            "Validation loss : 0.15008374\n",
            "Validation accuracy : 0.982\n",
            "At iteration 5000\n",
            "Training loss : 0.13963589\n",
            "Training accuracy : 0.9829\n",
            "Validation loss : 0.14803351\n",
            "Validation accuracy : 0.982\n",
            "At iteration 5100\n",
            "Training loss : 0.13764329\n",
            "Training accuracy : 0.983\n",
            "Validation loss : 0.1460668\n",
            "Validation accuracy : 0.982\n",
            "At iteration 5200\n",
            "Training loss : 0.13572848\n",
            "Training accuracy : 0.9831\n",
            "Validation loss : 0.14417763\n",
            "Validation accuracy : 0.982\n",
            "At iteration 5300\n",
            "Training loss : 0.1338878\n",
            "Training accuracy : 0.9834\n",
            "Validation loss : 0.14236242\n",
            "Validation accuracy : 0.982\n",
            "At iteration 5400\n",
            "Training loss : 0.13211684\n",
            "Training accuracy : 0.9836\n",
            "Validation loss : 0.14061372\n",
            "Validation accuracy : 0.984\n",
            "At iteration 5500\n",
            "Training loss : 0.13041107\n",
            "Training accuracy : 0.9837\n",
            "Validation loss : 0.13892867\n",
            "Validation accuracy : 0.984\n",
            "At iteration 5600\n",
            "Training loss : 0.12876783\n",
            "Training accuracy : 0.9835\n",
            "Validation loss : 0.13730098\n",
            "Validation accuracy : 0.984\n",
            "At iteration 5700\n",
            "Training loss : 0.12718177\n",
            "Training accuracy : 0.9836\n",
            "Validation loss : 0.13573125\n",
            "Validation accuracy : 0.984\n",
            "At iteration 5800\n",
            "Training loss : 0.12565231\n",
            "Training accuracy : 0.9836\n",
            "Validation loss : 0.13421576\n",
            "Validation accuracy : 0.984\n",
            "At iteration 5900\n",
            "Training loss : 0.12417588\n",
            "Training accuracy : 0.9838\n",
            "Validation loss : 0.1327498\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6000\n",
            "Training loss : 0.12275009\n",
            "Training accuracy : 0.9841\n",
            "Validation loss : 0.13132955\n",
            "Validation accuracy : 0.984\n",
            "At iteration 6100\n",
            "Training loss : 0.12137118\n",
            "Training accuracy : 0.9844\n",
            "Validation loss : 0.12995352\n",
            "Validation accuracy : 0.984\n",
            "At iteration 6200\n",
            "Training loss : 0.12003743\n",
            "Training accuracy : 0.9845\n",
            "Validation loss : 0.12862085\n",
            "Validation accuracy : 0.984\n",
            "At iteration 6300\n",
            "Training loss : 0.118746825\n",
            "Training accuracy : 0.9847\n",
            "Validation loss : 0.12733328\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6400\n",
            "Training loss : 0.117496274\n",
            "Training accuracy : 0.9845\n",
            "Validation loss : 0.12608601\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6500\n",
            "Training loss : 0.116282254\n",
            "Training accuracy : 0.9849\n",
            "Validation loss : 0.124876186\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6600\n",
            "Training loss : 0.115103066\n",
            "Training accuracy : 0.9849\n",
            "Validation loss : 0.123704724\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6700\n",
            "Training loss : 0.11395913\n",
            "Training accuracy : 0.9849\n",
            "Validation loss : 0.12257\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6800\n",
            "Training loss : 0.112847984\n",
            "Training accuracy : 0.9849\n",
            "Validation loss : 0.12147035\n",
            "Validation accuracy : 0.986\n",
            "At iteration 6900\n",
            "Training loss : 0.11176914\n",
            "Training accuracy : 0.9851\n",
            "Validation loss : 0.12039941\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7000\n",
            "Training loss : 0.11072022\n",
            "Training accuracy : 0.9851\n",
            "Validation loss : 0.119357996\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7100\n",
            "Training loss : 0.1096994\n",
            "Training accuracy : 0.9853\n",
            "Validation loss : 0.11834764\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7200\n",
            "Training loss : 0.10870591\n",
            "Training accuracy : 0.9853\n",
            "Validation loss : 0.117359646\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7300\n",
            "Training loss : 0.10773829\n",
            "Training accuracy : 0.9853\n",
            "Validation loss : 0.11639763\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7400\n",
            "Training loss : 0.10679596\n",
            "Training accuracy : 0.9853\n",
            "Validation loss : 0.1154586\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7500\n",
            "Training loss : 0.10587735\n",
            "Training accuracy : 0.9856\n",
            "Validation loss : 0.11454068\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7600\n",
            "Training loss : 0.10498244\n",
            "Training accuracy : 0.9856\n",
            "Validation loss : 0.11364596\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7700\n",
            "Training loss : 0.10411006\n",
            "Training accuracy : 0.9857\n",
            "Validation loss : 0.11276711\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7800\n",
            "Training loss : 0.10325781\n",
            "Training accuracy : 0.9858\n",
            "Validation loss : 0.11190781\n",
            "Validation accuracy : 0.988\n",
            "At iteration 7900\n",
            "Training loss : 0.1024272\n",
            "Training accuracy : 0.9859\n",
            "Validation loss : 0.11106644\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8000\n",
            "Training loss : 0.101616226\n",
            "Training accuracy : 0.986\n",
            "Validation loss : 0.11024623\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8100\n",
            "Training loss : 0.10082309\n",
            "Training accuracy : 0.9861\n",
            "Validation loss : 0.10944142\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8200\n",
            "Training loss : 0.10004764\n",
            "Training accuracy : 0.9862\n",
            "Validation loss : 0.10865206\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8300\n",
            "Training loss : 0.09929057\n",
            "Training accuracy : 0.9863\n",
            "Validation loss : 0.10788143\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8400\n",
            "Training loss : 0.09855022\n",
            "Training accuracy : 0.9863\n",
            "Validation loss : 0.10713387\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8500\n",
            "Training loss : 0.09782602\n",
            "Training accuracy : 0.9864\n",
            "Validation loss : 0.10640569\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8600\n",
            "Training loss : 0.09711788\n",
            "Training accuracy : 0.9865\n",
            "Validation loss : 0.105691634\n",
            "Validation accuracy : 0.988\n",
            "At iteration 8700\n",
            "Training loss : 0.096425176\n",
            "Training accuracy : 0.9866\n",
            "Validation loss : 0.10499307\n",
            "Validation accuracy : 0.99\n",
            "At iteration 8800\n",
            "Training loss : 0.09574844\n",
            "Training accuracy : 0.9868\n",
            "Validation loss : 0.1043079\n",
            "Validation accuracy : 0.99\n",
            "At iteration 8900\n",
            "Training loss : 0.095085256\n",
            "Training accuracy : 0.9869\n",
            "Validation loss : 0.10363755\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9000\n",
            "Training loss : 0.09443494\n",
            "Training accuracy : 0.9871\n",
            "Validation loss : 0.10297884\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9100\n",
            "Training loss : 0.093797326\n",
            "Training accuracy : 0.9872\n",
            "Validation loss : 0.10233539\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9200\n",
            "Training loss : 0.09317241\n",
            "Training accuracy : 0.9872\n",
            "Validation loss : 0.10170607\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9300\n",
            "Training loss : 0.09256017\n",
            "Training accuracy : 0.9874\n",
            "Validation loss : 0.10108653\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9400\n",
            "Training loss : 0.091959506\n",
            "Training accuracy : 0.9877\n",
            "Validation loss : 0.10047591\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9500\n",
            "Training loss : 0.091370046\n",
            "Training accuracy : 0.9877\n",
            "Validation loss : 0.099875845\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9600\n",
            "Training loss : 0.090792075\n",
            "Training accuracy : 0.9879\n",
            "Validation loss : 0.099287786\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9700\n",
            "Training loss : 0.09022531\n",
            "Training accuracy : 0.9879\n",
            "Validation loss : 0.098711066\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9800\n",
            "Training loss : 0.08966805\n",
            "Training accuracy : 0.9879\n",
            "Validation loss : 0.09814873\n",
            "Validation accuracy : 0.99\n",
            "At iteration 9900\n",
            "Training loss : 0.08912107\n",
            "Training accuracy : 0.9879\n",
            "Validation loss : 0.09760088\n",
            "Validation accuracy : 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UebI4tMq-tSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "373f6251-8a1d-467c-9cdc-0bf15bc6ec02"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training time :\",end-begin)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time : 30.61652970314026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WwPoMNHq-xlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9035
        },
        "outputId": "2ed40e37-eaa1-4aca-9b7a-67a5d1da052a"
      },
      "cell_type": "code",
      "source": [
        "# Let's try with 3 hidden neurons.\n",
        "model2 = generate_single_hidden_MLP(3) \n",
        "training_routine(model2,dataset,10000,gpu)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "At iteration 0\n",
            "Training loss : 0.75298476\n",
            "Training accuracy : 0.5849\n",
            "Validation loss : 0.76475996\n",
            "Validation accuracy : 0.566\n",
            "At iteration 100\n",
            "Training loss : 0.6855748\n",
            "Training accuracy : 0.6391\n",
            "Validation loss : 0.6891135\n",
            "Validation accuracy : 0.642\n",
            "At iteration 200\n",
            "Training loss : 0.66768086\n",
            "Training accuracy : 0.6178\n",
            "Validation loss : 0.66742885\n",
            "Validation accuracy : 0.624\n",
            "At iteration 300\n",
            "Training loss : 0.65896976\n",
            "Training accuracy : 0.6049\n",
            "Validation loss : 0.657211\n",
            "Validation accuracy : 0.608\n",
            "At iteration 400\n",
            "Training loss : 0.6524156\n",
            "Training accuracy : 0.6031\n",
            "Validation loss : 0.65009236\n",
            "Validation accuracy : 0.606\n",
            "At iteration 500\n",
            "Training loss : 0.6468379\n",
            "Training accuracy : 0.6042\n",
            "Validation loss : 0.64434516\n",
            "Validation accuracy : 0.608\n",
            "At iteration 600\n",
            "Training loss : 0.64213175\n",
            "Training accuracy : 0.6059\n",
            "Validation loss : 0.6394472\n",
            "Validation accuracy : 0.606\n",
            "At iteration 700\n",
            "Training loss : 0.6379722\n",
            "Training accuracy : 0.6094\n",
            "Validation loss : 0.635187\n",
            "Validation accuracy : 0.612\n",
            "At iteration 800\n",
            "Training loss : 0.63408625\n",
            "Training accuracy : 0.6136\n",
            "Validation loss : 0.6312929\n",
            "Validation accuracy : 0.612\n",
            "At iteration 900\n",
            "Training loss : 0.6303876\n",
            "Training accuracy : 0.6159\n",
            "Validation loss : 0.62762797\n",
            "Validation accuracy : 0.62\n",
            "At iteration 1000\n",
            "Training loss : 0.62682617\n",
            "Training accuracy : 0.6187\n",
            "Validation loss : 0.6241775\n",
            "Validation accuracy : 0.624\n",
            "At iteration 1100\n",
            "Training loss : 0.62330383\n",
            "Training accuracy : 0.6213\n",
            "Validation loss : 0.6207328\n",
            "Validation accuracy : 0.63\n",
            "At iteration 1200\n",
            "Training loss : 0.6197632\n",
            "Training accuracy : 0.624\n",
            "Validation loss : 0.6172698\n",
            "Validation accuracy : 0.626\n",
            "At iteration 1300\n",
            "Training loss : 0.6161406\n",
            "Training accuracy : 0.6277\n",
            "Validation loss : 0.613765\n",
            "Validation accuracy : 0.632\n",
            "At iteration 1400\n",
            "Training loss : 0.6123199\n",
            "Training accuracy : 0.6296\n",
            "Validation loss : 0.6101245\n",
            "Validation accuracy : 0.632\n",
            "At iteration 1500\n",
            "Training loss : 0.60818386\n",
            "Training accuracy : 0.6321\n",
            "Validation loss : 0.6062257\n",
            "Validation accuracy : 0.636\n",
            "At iteration 1600\n",
            "Training loss : 0.6036381\n",
            "Training accuracy : 0.6343\n",
            "Validation loss : 0.60195273\n",
            "Validation accuracy : 0.642\n",
            "At iteration 1700\n",
            "Training loss : 0.5986355\n",
            "Training accuracy : 0.6367\n",
            "Validation loss : 0.59728456\n",
            "Validation accuracy : 0.642\n",
            "At iteration 1800\n",
            "Training loss : 0.5931543\n",
            "Training accuracy : 0.6381\n",
            "Validation loss : 0.5921311\n",
            "Validation accuracy : 0.64\n",
            "At iteration 1900\n",
            "Training loss : 0.5871609\n",
            "Training accuracy : 0.6393\n",
            "Validation loss : 0.5865616\n",
            "Validation accuracy : 0.64\n",
            "At iteration 2000\n",
            "Training loss : 0.58059585\n",
            "Training accuracy : 0.6419\n",
            "Validation loss : 0.5804603\n",
            "Validation accuracy : 0.654\n",
            "At iteration 2100\n",
            "Training loss : 0.5733594\n",
            "Training accuracy : 0.6427\n",
            "Validation loss : 0.57363695\n",
            "Validation accuracy : 0.656\n",
            "At iteration 2200\n",
            "Training loss : 0.56536645\n",
            "Training accuracy : 0.6448\n",
            "Validation loss : 0.5659446\n",
            "Validation accuracy : 0.658\n",
            "At iteration 2300\n",
            "Training loss : 0.55661213\n",
            "Training accuracy : 0.6643\n",
            "Validation loss : 0.55743605\n",
            "Validation accuracy : 0.676\n",
            "At iteration 2400\n",
            "Training loss : 0.5469891\n",
            "Training accuracy : 0.6889\n",
            "Validation loss : 0.54812884\n",
            "Validation accuracy : 0.7\n",
            "At iteration 2500\n",
            "Training loss : 0.5365626\n",
            "Training accuracy : 0.7174\n",
            "Validation loss : 0.538029\n",
            "Validation accuracy : 0.722\n",
            "At iteration 2600\n",
            "Training loss : 0.525364\n",
            "Training accuracy : 0.7396\n",
            "Validation loss : 0.5271644\n",
            "Validation accuracy : 0.75\n",
            "At iteration 2700\n",
            "Training loss : 0.51348335\n",
            "Training accuracy : 0.758\n",
            "Validation loss : 0.51547474\n",
            "Validation accuracy : 0.77\n",
            "At iteration 2800\n",
            "Training loss : 0.50103927\n",
            "Training accuracy : 0.7756\n",
            "Validation loss : 0.5032877\n",
            "Validation accuracy : 0.79\n",
            "At iteration 2900\n",
            "Training loss : 0.48816958\n",
            "Training accuracy : 0.7853\n",
            "Validation loss : 0.49072886\n",
            "Validation accuracy : 0.798\n",
            "At iteration 3000\n",
            "Training loss : 0.47498676\n",
            "Training accuracy : 0.7923\n",
            "Validation loss : 0.4779614\n",
            "Validation accuracy : 0.796\n",
            "At iteration 3100\n",
            "Training loss : 0.46174073\n",
            "Training accuracy : 0.846\n",
            "Validation loss : 0.46509686\n",
            "Validation accuracy : 0.858\n",
            "At iteration 3200\n",
            "Training loss : 0.44854322\n",
            "Training accuracy : 0.851\n",
            "Validation loss : 0.45209372\n",
            "Validation accuracy : 0.856\n",
            "At iteration 3300\n",
            "Training loss : 0.43549848\n",
            "Training accuracy : 0.8557\n",
            "Validation loss : 0.4392094\n",
            "Validation accuracy : 0.856\n",
            "At iteration 3400\n",
            "Training loss : 0.4227958\n",
            "Training accuracy : 0.8614\n",
            "Validation loss : 0.42640337\n",
            "Validation accuracy : 0.858\n",
            "At iteration 3500\n",
            "Training loss : 0.41052574\n",
            "Training accuracy : 0.8674\n",
            "Validation loss : 0.41399777\n",
            "Validation accuracy : 0.868\n",
            "At iteration 3600\n",
            "Training loss : 0.39868468\n",
            "Training accuracy : 0.8721\n",
            "Validation loss : 0.40201095\n",
            "Validation accuracy : 0.874\n",
            "At iteration 3700\n",
            "Training loss : 0.3873141\n",
            "Training accuracy : 0.8763\n",
            "Validation loss : 0.39062348\n",
            "Validation accuracy : 0.88\n",
            "At iteration 3800\n",
            "Training loss : 0.37638807\n",
            "Training accuracy : 0.8803\n",
            "Validation loss : 0.37969205\n",
            "Validation accuracy : 0.886\n",
            "At iteration 3900\n",
            "Training loss : 0.3659077\n",
            "Training accuracy : 0.885\n",
            "Validation loss : 0.3692246\n",
            "Validation accuracy : 0.89\n",
            "At iteration 4000\n",
            "Training loss : 0.35586926\n",
            "Training accuracy : 0.8901\n",
            "Validation loss : 0.35917833\n",
            "Validation accuracy : 0.898\n",
            "At iteration 4100\n",
            "Training loss : 0.3462677\n",
            "Training accuracy : 0.8954\n",
            "Validation loss : 0.34955367\n",
            "Validation accuracy : 0.904\n",
            "At iteration 4200\n",
            "Training loss : 0.33709076\n",
            "Training accuracy : 0.9\n",
            "Validation loss : 0.3403302\n",
            "Validation accuracy : 0.906\n",
            "At iteration 4300\n",
            "Training loss : 0.32833293\n",
            "Training accuracy : 0.9038\n",
            "Validation loss : 0.3315156\n",
            "Validation accuracy : 0.91\n",
            "At iteration 4400\n",
            "Training loss : 0.31997475\n",
            "Training accuracy : 0.9075\n",
            "Validation loss : 0.32313657\n",
            "Validation accuracy : 0.912\n",
            "At iteration 4500\n",
            "Training loss : 0.31200257\n",
            "Training accuracy : 0.9114\n",
            "Validation loss : 0.31517148\n",
            "Validation accuracy : 0.914\n",
            "At iteration 4600\n",
            "Training loss : 0.30441737\n",
            "Training accuracy : 0.9155\n",
            "Validation loss : 0.30756363\n",
            "Validation accuracy : 0.92\n",
            "At iteration 4700\n",
            "Training loss : 0.29719928\n",
            "Training accuracy : 0.9189\n",
            "Validation loss : 0.30030662\n",
            "Validation accuracy : 0.922\n",
            "At iteration 4800\n",
            "Training loss : 0.29030278\n",
            "Training accuracy : 0.922\n",
            "Validation loss : 0.29340523\n",
            "Validation accuracy : 0.922\n",
            "At iteration 4900\n",
            "Training loss : 0.28374666\n",
            "Training accuracy : 0.9243\n",
            "Validation loss : 0.28682563\n",
            "Validation accuracy : 0.924\n",
            "At iteration 5000\n",
            "Training loss : 0.27747726\n",
            "Training accuracy : 0.9269\n",
            "Validation loss : 0.28056303\n",
            "Validation accuracy : 0.926\n",
            "At iteration 5100\n",
            "Training loss : 0.2714884\n",
            "Training accuracy : 0.9292\n",
            "Validation loss : 0.2746195\n",
            "Validation accuracy : 0.928\n",
            "At iteration 5200\n",
            "Training loss : 0.2657562\n",
            "Training accuracy : 0.9317\n",
            "Validation loss : 0.26894853\n",
            "Validation accuracy : 0.928\n",
            "At iteration 5300\n",
            "Training loss : 0.26027617\n",
            "Training accuracy : 0.9334\n",
            "Validation loss : 0.26354924\n",
            "Validation accuracy : 0.932\n",
            "At iteration 5400\n",
            "Training loss : 0.25505868\n",
            "Training accuracy : 0.9349\n",
            "Validation loss : 0.25839737\n",
            "Validation accuracy : 0.936\n",
            "At iteration 5500\n",
            "Training loss : 0.25005418\n",
            "Training accuracy : 0.9383\n",
            "Validation loss : 0.25347927\n",
            "Validation accuracy : 0.938\n",
            "At iteration 5600\n",
            "Training loss : 0.2452529\n",
            "Training accuracy : 0.9401\n",
            "Validation loss : 0.24877043\n",
            "Validation accuracy : 0.944\n",
            "At iteration 5700\n",
            "Training loss : 0.24065882\n",
            "Training accuracy : 0.9414\n",
            "Validation loss : 0.24429357\n",
            "Validation accuracy : 0.948\n",
            "At iteration 5800\n",
            "Training loss : 0.23624602\n",
            "Training accuracy : 0.9426\n",
            "Validation loss : 0.24002923\n",
            "Validation accuracy : 0.95\n",
            "At iteration 5900\n",
            "Training loss : 0.23201609\n",
            "Training accuracy : 0.9438\n",
            "Validation loss : 0.23596455\n",
            "Validation accuracy : 0.95\n",
            "At iteration 6000\n",
            "Training loss : 0.2279627\n",
            "Training accuracy : 0.9451\n",
            "Validation loss : 0.23208019\n",
            "Validation accuracy : 0.952\n",
            "At iteration 6100\n",
            "Training loss : 0.22408576\n",
            "Training accuracy : 0.9457\n",
            "Validation loss : 0.2283833\n",
            "Validation accuracy : 0.954\n",
            "At iteration 6200\n",
            "Training loss : 0.22037044\n",
            "Training accuracy : 0.947\n",
            "Validation loss : 0.22487466\n",
            "Validation accuracy : 0.954\n",
            "At iteration 6300\n",
            "Training loss : 0.21680123\n",
            "Training accuracy : 0.9484\n",
            "Validation loss : 0.2215199\n",
            "Validation accuracy : 0.954\n",
            "At iteration 6400\n",
            "Training loss : 0.2133762\n",
            "Training accuracy : 0.9492\n",
            "Validation loss : 0.21830378\n",
            "Validation accuracy : 0.954\n",
            "At iteration 6500\n",
            "Training loss : 0.2100837\n",
            "Training accuracy : 0.9495\n",
            "Validation loss : 0.21521658\n",
            "Validation accuracy : 0.952\n",
            "At iteration 6600\n",
            "Training loss : 0.20691347\n",
            "Training accuracy : 0.9499\n",
            "Validation loss : 0.21224436\n",
            "Validation accuracy : 0.948\n",
            "At iteration 6700\n",
            "Training loss : 0.20386626\n",
            "Training accuracy : 0.9512\n",
            "Validation loss : 0.20937425\n",
            "Validation accuracy : 0.948\n",
            "At iteration 6800\n",
            "Training loss : 0.20094763\n",
            "Training accuracy : 0.9518\n",
            "Validation loss : 0.2065901\n",
            "Validation accuracy : 0.948\n",
            "At iteration 6900\n",
            "Training loss : 0.1981462\n",
            "Training accuracy : 0.9528\n",
            "Validation loss : 0.20390382\n",
            "Validation accuracy : 0.948\n",
            "At iteration 7000\n",
            "Training loss : 0.195461\n",
            "Training accuracy : 0.953\n",
            "Validation loss : 0.20128271\n",
            "Validation accuracy : 0.948\n",
            "At iteration 7100\n",
            "Training loss : 0.19287732\n",
            "Training accuracy : 0.9528\n",
            "Validation loss : 0.19875127\n",
            "Validation accuracy : 0.948\n",
            "At iteration 7200\n",
            "Training loss : 0.19039172\n",
            "Training accuracy : 0.9532\n",
            "Validation loss : 0.19632019\n",
            "Validation accuracy : 0.95\n",
            "At iteration 7300\n",
            "Training loss : 0.18801029\n",
            "Training accuracy : 0.9533\n",
            "Validation loss : 0.19396271\n",
            "Validation accuracy : 0.95\n",
            "At iteration 7400\n",
            "Training loss : 0.18571295\n",
            "Training accuracy : 0.9533\n",
            "Validation loss : 0.19168156\n",
            "Validation accuracy : 0.952\n",
            "At iteration 7500\n",
            "Training loss : 0.18349981\n",
            "Training accuracy : 0.9533\n",
            "Validation loss : 0.18948025\n",
            "Validation accuracy : 0.952\n",
            "At iteration 7600\n",
            "Training loss : 0.1813623\n",
            "Training accuracy : 0.9533\n",
            "Validation loss : 0.18736394\n",
            "Validation accuracy : 0.952\n",
            "At iteration 7700\n",
            "Training loss : 0.17930216\n",
            "Training accuracy : 0.9538\n",
            "Validation loss : 0.18532641\n",
            "Validation accuracy : 0.952\n",
            "At iteration 7800\n",
            "Training loss : 0.17730775\n",
            "Training accuracy : 0.9537\n",
            "Validation loss : 0.1833638\n",
            "Validation accuracy : 0.95\n",
            "At iteration 7900\n",
            "Training loss : 0.17538692\n",
            "Training accuracy : 0.9541\n",
            "Validation loss : 0.18145952\n",
            "Validation accuracy : 0.95\n",
            "At iteration 8000\n",
            "Training loss : 0.1735317\n",
            "Training accuracy : 0.9538\n",
            "Validation loss : 0.17962278\n",
            "Validation accuracy : 0.952\n",
            "At iteration 8100\n",
            "Training loss : 0.17173898\n",
            "Training accuracy : 0.9536\n",
            "Validation loss : 0.17783658\n",
            "Validation accuracy : 0.95\n",
            "At iteration 8200\n",
            "Training loss : 0.17000708\n",
            "Training accuracy : 0.9538\n",
            "Validation loss : 0.17611614\n",
            "Validation accuracy : 0.95\n",
            "At iteration 8300\n",
            "Training loss : 0.16833243\n",
            "Training accuracy : 0.9543\n",
            "Validation loss : 0.17443153\n",
            "Validation accuracy : 0.95\n",
            "At iteration 8400\n",
            "Training loss : 0.16671622\n",
            "Training accuracy : 0.9546\n",
            "Validation loss : 0.17279431\n",
            "Validation accuracy : 0.95\n",
            "At iteration 8500\n",
            "Training loss : 0.16514967\n",
            "Training accuracy : 0.9543\n",
            "Validation loss : 0.17120685\n",
            "Validation accuracy : 0.95\n",
            "At iteration 8600\n",
            "Training loss : 0.16363506\n",
            "Training accuracy : 0.9544\n",
            "Validation loss : 0.16967703\n",
            "Validation accuracy : 0.952\n",
            "At iteration 8700\n",
            "Training loss : 0.16217361\n",
            "Training accuracy : 0.9543\n",
            "Validation loss : 0.16819893\n",
            "Validation accuracy : 0.952\n",
            "At iteration 8800\n",
            "Training loss : 0.1607544\n",
            "Training accuracy : 0.9544\n",
            "Validation loss : 0.16675927\n",
            "Validation accuracy : 0.952\n",
            "At iteration 8900\n",
            "Training loss : 0.15937747\n",
            "Training accuracy : 0.9544\n",
            "Validation loss : 0.16536616\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9000\n",
            "Training loss : 0.15804373\n",
            "Training accuracy : 0.9549\n",
            "Validation loss : 0.16402155\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9100\n",
            "Training loss : 0.1567532\n",
            "Training accuracy : 0.9549\n",
            "Validation loss : 0.16271399\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9200\n",
            "Training loss : 0.1554998\n",
            "Training accuracy : 0.955\n",
            "Validation loss : 0.16144697\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9300\n",
            "Training loss : 0.15428124\n",
            "Training accuracy : 0.9547\n",
            "Validation loss : 0.16021222\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9400\n",
            "Training loss : 0.15309754\n",
            "Training accuracy : 0.9546\n",
            "Validation loss : 0.15901332\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9500\n",
            "Training loss : 0.15195043\n",
            "Training accuracy : 0.9547\n",
            "Validation loss : 0.15784918\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9600\n",
            "Training loss : 0.15083246\n",
            "Training accuracy : 0.9546\n",
            "Validation loss : 0.1567144\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9700\n",
            "Training loss : 0.14974634\n",
            "Training accuracy : 0.9548\n",
            "Validation loss : 0.15561026\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9800\n",
            "Training loss : 0.14868794\n",
            "Training accuracy : 0.9548\n",
            "Validation loss : 0.15453874\n",
            "Validation accuracy : 0.952\n",
            "At iteration 9900\n",
            "Training loss : 0.14766075\n",
            "Training accuracy : 0.9547\n",
            "Validation loss : 0.15349968\n",
            "Validation accuracy : 0.954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p4xu3n9S-y-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "dadb4cca-a674-44cb-943a-117cf1e403cd"
      },
      "cell_type": "code",
      "source": [
        "out = model2(testx).argmax(dim=1).detach().numpy()\n",
        "green = testx.numpy()[np.where(out==1)]\n",
        "red = testx.numpy()[np.where(out==0)]\n",
        "print(green.shape,red.shape)|"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-f0097643ff04>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(green.shape,red.shape)|\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GKhKTQqn-0rJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_model(model,datapoints):\n",
        "    out = model(datapoints).argmax(dim=1).detach().numpy()\n",
        "    green = datapoints.numpy()[np.where(out==1)]\n",
        "    red = datapoints.numpy()[np.where(out==0)]\n",
        "\n",
        "    circle1 = plt.Circle((0, 0), 1, color='y')\n",
        "    circle2 = plt.Circle((0, 0), 1, color='b',fill=False)\n",
        "\n",
        "    fig, ax = plt.subplots() # note we must use plt.subplots, not plt.subplot\n",
        "    # (or if you have an existing figure)\n",
        "    # fig = plt.gcf()\n",
        "    # ax = fig.gca()\n",
        "    plt.xlim((-2,2))\n",
        "    plt.ylim((-2,2))\n",
        "\n",
        "    pos_values = plt.scatter(x=green[:,0],y=green[:,1], color='g',)\n",
        "    neg_values = plt.scatter(x=red[:,0],y=red[:,1], color='r',)\n",
        "\n",
        "    ax.add_artist(circle1)\n",
        "    ax.add_artist(circle2)\n",
        "    ax.add_artist(pos_values)\n",
        "    ax.add_artist(neg_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6Z3SNtq-5_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "aa20026c-9b6d-4aca-c2d8-b8d35138a2aa"
      },
      "cell_type": "code",
      "source": [
        "print_model(model2,testx)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzDXLoyymTwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdca15b4748>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "062Kv3C8-800",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9035
        },
        "outputId": "b1fd51f5-8528-4eb1-86e1-a3e108809355"
      },
      "cell_type": "code",
      "source": [
        "model3 = generate_single_hidden_MLP(2) \n",
        "training_routine(model3,dataset,10000,gpu)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "At iteration 0\n",
            "Training loss : 0.86396515\n",
            "Training accuracy : 0.5035\n",
            "Validation loss : 0.8452978\n",
            "Validation accuracy : 0.528\n",
            "At iteration 100\n",
            "Training loss : 0.7457385\n",
            "Training accuracy : 0.5035\n",
            "Validation loss : 0.7345665\n",
            "Validation accuracy : 0.528\n",
            "At iteration 200\n",
            "Training loss : 0.6985501\n",
            "Training accuracy : 0.4811\n",
            "Validation loss : 0.6927115\n",
            "Validation accuracy : 0.496\n",
            "At iteration 300\n",
            "Training loss : 0.67594385\n",
            "Training accuracy : 0.4183\n",
            "Validation loss : 0.67345244\n",
            "Validation accuracy : 0.442\n",
            "At iteration 400\n",
            "Training loss : 0.6628801\n",
            "Training accuracy : 0.3542\n",
            "Validation loss : 0.66265297\n",
            "Validation accuracy : 0.376\n",
            "At iteration 500\n",
            "Training loss : 0.65406895\n",
            "Training accuracy : 0.5862\n",
            "Validation loss : 0.6555081\n",
            "Validation accuracy : 0.564\n",
            "At iteration 600\n",
            "Training loss : 0.64759135\n",
            "Training accuracy : 0.5983\n",
            "Validation loss : 0.65024763\n",
            "Validation accuracy : 0.568\n",
            "At iteration 700\n",
            "Training loss : 0.6424451\n",
            "Training accuracy : 0.6065\n",
            "Validation loss : 0.6461449\n",
            "Validation accuracy : 0.568\n",
            "At iteration 800\n",
            "Training loss : 0.63830084\n",
            "Training accuracy : 0.6101\n",
            "Validation loss : 0.6428392\n",
            "Validation accuracy : 0.574\n",
            "At iteration 900\n",
            "Training loss : 0.63482004\n",
            "Training accuracy : 0.612\n",
            "Validation loss : 0.6400806\n",
            "Validation accuracy : 0.582\n",
            "At iteration 1000\n",
            "Training loss : 0.63175875\n",
            "Training accuracy : 0.6129\n",
            "Validation loss : 0.637676\n",
            "Validation accuracy : 0.584\n",
            "At iteration 1100\n",
            "Training loss : 0.6289593\n",
            "Training accuracy : 0.6146\n",
            "Validation loss : 0.6354508\n",
            "Validation accuracy : 0.584\n",
            "At iteration 1200\n",
            "Training loss : 0.62621605\n",
            "Training accuracy : 0.6162\n",
            "Validation loss : 0.63325304\n",
            "Validation accuracy : 0.586\n",
            "At iteration 1300\n",
            "Training loss : 0.6234523\n",
            "Training accuracy : 0.617\n",
            "Validation loss : 0.63090545\n",
            "Validation accuracy : 0.586\n",
            "At iteration 1400\n",
            "Training loss : 0.62028503\n",
            "Training accuracy : 0.6177\n",
            "Validation loss : 0.628194\n",
            "Validation accuracy : 0.582\n",
            "At iteration 1500\n",
            "Training loss : 0.6166485\n",
            "Training accuracy : 0.6191\n",
            "Validation loss : 0.6248357\n",
            "Validation accuracy : 0.586\n",
            "At iteration 1600\n",
            "Training loss : 0.6120538\n",
            "Training accuracy : 0.6201\n",
            "Validation loss : 0.6204718\n",
            "Validation accuracy : 0.584\n",
            "At iteration 1700\n",
            "Training loss : 0.6062128\n",
            "Training accuracy : 0.6208\n",
            "Validation loss : 0.6147057\n",
            "Validation accuracy : 0.588\n",
            "At iteration 1800\n",
            "Training loss : 0.59887415\n",
            "Training accuracy : 0.6442\n",
            "Validation loss : 0.6071966\n",
            "Validation accuracy : 0.618\n",
            "At iteration 1900\n",
            "Training loss : 0.5898449\n",
            "Training accuracy : 0.6927\n",
            "Validation loss : 0.5979214\n",
            "Validation accuracy : 0.672\n",
            "At iteration 2000\n",
            "Training loss : 0.5794568\n",
            "Training accuracy : 0.7374\n",
            "Validation loss : 0.5871319\n",
            "Validation accuracy : 0.714\n",
            "At iteration 2100\n",
            "Training loss : 0.56812733\n",
            "Training accuracy : 0.7705\n",
            "Validation loss : 0.5754028\n",
            "Validation accuracy : 0.736\n",
            "At iteration 2200\n",
            "Training loss : 0.55652225\n",
            "Training accuracy : 0.7818\n",
            "Validation loss : 0.5633951\n",
            "Validation accuracy : 0.754\n",
            "At iteration 2300\n",
            "Training loss : 0.5452565\n",
            "Training accuracy : 0.783\n",
            "Validation loss : 0.5518219\n",
            "Validation accuracy : 0.768\n",
            "At iteration 2400\n",
            "Training loss : 0.53469205\n",
            "Training accuracy : 0.784\n",
            "Validation loss : 0.5411255\n",
            "Validation accuracy : 0.766\n",
            "At iteration 2500\n",
            "Training loss : 0.5249571\n",
            "Training accuracy : 0.7839\n",
            "Validation loss : 0.53142244\n",
            "Validation accuracy : 0.77\n",
            "At iteration 2600\n",
            "Training loss : 0.51614016\n",
            "Training accuracy : 0.7831\n",
            "Validation loss : 0.5227577\n",
            "Validation accuracy : 0.77\n",
            "At iteration 2700\n",
            "Training loss : 0.508193\n",
            "Training accuracy : 0.7831\n",
            "Validation loss : 0.5151012\n",
            "Validation accuracy : 0.77\n",
            "At iteration 2800\n",
            "Training loss : 0.5010928\n",
            "Training accuracy : 0.7836\n",
            "Validation loss : 0.5083703\n",
            "Validation accuracy : 0.768\n",
            "At iteration 2900\n",
            "Training loss : 0.49480805\n",
            "Training accuracy : 0.7826\n",
            "Validation loss : 0.50250304\n",
            "Validation accuracy : 0.766\n",
            "At iteration 3000\n",
            "Training loss : 0.48924574\n",
            "Training accuracy : 0.783\n",
            "Validation loss : 0.49739814\n",
            "Validation accuracy : 0.768\n",
            "At iteration 3100\n",
            "Training loss : 0.48433858\n",
            "Training accuracy : 0.7822\n",
            "Validation loss : 0.4929382\n",
            "Validation accuracy : 0.768\n",
            "At iteration 3200\n",
            "Training loss : 0.4800336\n",
            "Training accuracy : 0.7824\n",
            "Validation loss : 0.48905292\n",
            "Validation accuracy : 0.772\n",
            "At iteration 3300\n",
            "Training loss : 0.47625458\n",
            "Training accuracy : 0.7821\n",
            "Validation loss : 0.4856854\n",
            "Validation accuracy : 0.77\n",
            "At iteration 3400\n",
            "Training loss : 0.47293296\n",
            "Training accuracy : 0.7821\n",
            "Validation loss : 0.48279184\n",
            "Validation accuracy : 0.77\n",
            "At iteration 3500\n",
            "Training loss : 0.4700137\n",
            "Training accuracy : 0.7813\n",
            "Validation loss : 0.4803183\n",
            "Validation accuracy : 0.772\n",
            "At iteration 3600\n",
            "Training loss : 0.4674324\n",
            "Training accuracy : 0.7813\n",
            "Validation loss : 0.4781552\n",
            "Validation accuracy : 0.772\n",
            "At iteration 3700\n",
            "Training loss : 0.46517256\n",
            "Training accuracy : 0.7818\n",
            "Validation loss : 0.47627544\n",
            "Validation accuracy : 0.772\n",
            "At iteration 3800\n",
            "Training loss : 0.46319318\n",
            "Training accuracy : 0.7813\n",
            "Validation loss : 0.47464117\n",
            "Validation accuracy : 0.77\n",
            "At iteration 3900\n",
            "Training loss : 0.46142763\n",
            "Training accuracy : 0.782\n",
            "Validation loss : 0.47323683\n",
            "Validation accuracy : 0.766\n",
            "At iteration 4000\n",
            "Training loss : 0.45989132\n",
            "Training accuracy : 0.7817\n",
            "Validation loss : 0.47202268\n",
            "Validation accuracy : 0.768\n",
            "At iteration 4100\n",
            "Training loss : 0.4585271\n",
            "Training accuracy : 0.7817\n",
            "Validation loss : 0.47097263\n",
            "Validation accuracy : 0.768\n",
            "At iteration 4200\n",
            "Training loss : 0.45733243\n",
            "Training accuracy : 0.7818\n",
            "Validation loss : 0.47008935\n",
            "Validation accuracy : 0.766\n",
            "At iteration 4300\n",
            "Training loss : 0.45625338\n",
            "Training accuracy : 0.7814\n",
            "Validation loss : 0.46935043\n",
            "Validation accuracy : 0.766\n",
            "At iteration 4400\n",
            "Training loss : 0.455291\n",
            "Training accuracy : 0.7814\n",
            "Validation loss : 0.4687275\n",
            "Validation accuracy : 0.764\n",
            "At iteration 4500\n",
            "Training loss : 0.4544557\n",
            "Training accuracy : 0.7812\n",
            "Validation loss : 0.46818763\n",
            "Validation accuracy : 0.764\n",
            "At iteration 4600\n",
            "Training loss : 0.4536917\n",
            "Training accuracy : 0.7811\n",
            "Validation loss : 0.4677391\n",
            "Validation accuracy : 0.764\n",
            "At iteration 4700\n",
            "Training loss : 0.4530226\n",
            "Training accuracy : 0.7813\n",
            "Validation loss : 0.467354\n",
            "Validation accuracy : 0.762\n",
            "At iteration 4800\n",
            "Training loss : 0.45241314\n",
            "Training accuracy : 0.7813\n",
            "Validation loss : 0.46703243\n",
            "Validation accuracy : 0.762\n",
            "At iteration 4900\n",
            "Training loss : 0.4518749\n",
            "Training accuracy : 0.7809\n",
            "Validation loss : 0.46676806\n",
            "Validation accuracy : 0.762\n",
            "At iteration 5000\n",
            "Training loss : 0.45138907\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46655256\n",
            "Validation accuracy : 0.766\n",
            "At iteration 5100\n",
            "Training loss : 0.45095932\n",
            "Training accuracy : 0.781\n",
            "Validation loss : 0.46639565\n",
            "Validation accuracy : 0.768\n",
            "At iteration 5200\n",
            "Training loss : 0.45057806\n",
            "Training accuracy : 0.7812\n",
            "Validation loss : 0.4662765\n",
            "Validation accuracy : 0.768\n",
            "At iteration 5300\n",
            "Training loss : 0.45020792\n",
            "Training accuracy : 0.781\n",
            "Validation loss : 0.46617118\n",
            "Validation accuracy : 0.768\n",
            "At iteration 5400\n",
            "Training loss : 0.44988292\n",
            "Training accuracy : 0.7811\n",
            "Validation loss : 0.46604764\n",
            "Validation accuracy : 0.764\n",
            "At iteration 5500\n",
            "Training loss : 0.44959435\n",
            "Training accuracy : 0.7809\n",
            "Validation loss : 0.4659448\n",
            "Validation accuracy : 0.764\n",
            "At iteration 5600\n",
            "Training loss : 0.4493461\n",
            "Training accuracy : 0.7812\n",
            "Validation loss : 0.465891\n",
            "Validation accuracy : 0.764\n",
            "At iteration 5700\n",
            "Training loss : 0.44908336\n",
            "Training accuracy : 0.7816\n",
            "Validation loss : 0.46583432\n",
            "Validation accuracy : 0.764\n",
            "At iteration 5800\n",
            "Training loss : 0.44889042\n",
            "Training accuracy : 0.781\n",
            "Validation loss : 0.4657508\n",
            "Validation accuracy : 0.764\n",
            "At iteration 5900\n",
            "Training loss : 0.44867724\n",
            "Training accuracy : 0.781\n",
            "Validation loss : 0.46565917\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6000\n",
            "Training loss : 0.4485148\n",
            "Training accuracy : 0.7809\n",
            "Validation loss : 0.4655853\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6100\n",
            "Training loss : 0.44833583\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46552\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6200\n",
            "Training loss : 0.44818667\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46548516\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6300\n",
            "Training loss : 0.4480369\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46546075\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6400\n",
            "Training loss : 0.44792783\n",
            "Training accuracy : 0.7807\n",
            "Validation loss : 0.46543077\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6500\n",
            "Training loss : 0.44781485\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46540985\n",
            "Validation accuracy : 0.764\n",
            "At iteration 6600\n",
            "Training loss : 0.44771323\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.46537384\n",
            "Validation accuracy : 0.762\n",
            "At iteration 6700\n",
            "Training loss : 0.447615\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.46535093\n",
            "Validation accuracy : 0.762\n",
            "At iteration 6800\n",
            "Training loss : 0.4475214\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.46533793\n",
            "Validation accuracy : 0.762\n",
            "At iteration 6900\n",
            "Training loss : 0.44742295\n",
            "Training accuracy : 0.7802\n",
            "Validation loss : 0.46532828\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7000\n",
            "Training loss : 0.44735482\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.46531722\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7100\n",
            "Training loss : 0.44727597\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.4652982\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7200\n",
            "Training loss : 0.44720396\n",
            "Training accuracy : 0.7802\n",
            "Validation loss : 0.46527556\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7300\n",
            "Training loss : 0.44712847\n",
            "Training accuracy : 0.7801\n",
            "Validation loss : 0.4652579\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7400\n",
            "Training loss : 0.44710073\n",
            "Training accuracy : 0.7801\n",
            "Validation loss : 0.46525148\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7500\n",
            "Training loss : 0.4470341\n",
            "Training accuracy : 0.78\n",
            "Validation loss : 0.4652573\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7600\n",
            "Training loss : 0.44697973\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.46527722\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7700\n",
            "Training loss : 0.44693246\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.46527737\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7800\n",
            "Training loss : 0.44688165\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.4652624\n",
            "Validation accuracy : 0.762\n",
            "At iteration 7900\n",
            "Training loss : 0.44683257\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46523577\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8000\n",
            "Training loss : 0.4467889\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46520084\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8100\n",
            "Training loss : 0.44673204\n",
            "Training accuracy : 0.7807\n",
            "Validation loss : 0.46518472\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8200\n",
            "Training loss : 0.4466923\n",
            "Training accuracy : 0.7807\n",
            "Validation loss : 0.46515486\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8300\n",
            "Training loss : 0.4466665\n",
            "Training accuracy : 0.7807\n",
            "Validation loss : 0.4651309\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8400\n",
            "Training loss : 0.44660366\n",
            "Training accuracy : 0.7806\n",
            "Validation loss : 0.46510148\n",
            "Validation accuracy : 0.76\n",
            "At iteration 8500\n",
            "Training loss : 0.44657674\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.46506903\n",
            "Validation accuracy : 0.76\n",
            "At iteration 8600\n",
            "Training loss : 0.4465438\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.46503714\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8700\n",
            "Training loss : 0.44650263\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.4650011\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8800\n",
            "Training loss : 0.4464587\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.46496904\n",
            "Validation accuracy : 0.762\n",
            "At iteration 8900\n",
            "Training loss : 0.44644296\n",
            "Training accuracy : 0.7803\n",
            "Validation loss : 0.4649443\n",
            "Validation accuracy : 0.762\n",
            "At iteration 9000\n",
            "Training loss : 0.4464105\n",
            "Training accuracy : 0.7804\n",
            "Validation loss : 0.4649253\n",
            "Validation accuracy : 0.762\n",
            "At iteration 9100\n",
            "Training loss : 0.44637343\n",
            "Training accuracy : 0.7805\n",
            "Validation loss : 0.46489227\n",
            "Validation accuracy : 0.762\n",
            "At iteration 9200\n",
            "Training loss : 0.44634375\n",
            "Training accuracy : 0.7805\n",
            "Validation loss : 0.46486402\n",
            "Validation accuracy : 0.762\n",
            "At iteration 9300\n",
            "Training loss : 0.44630933\n",
            "Training accuracy : 0.7805\n",
            "Validation loss : 0.4648106\n",
            "Validation accuracy : 0.762\n",
            "At iteration 9400\n",
            "Training loss : 0.44627348\n",
            "Training accuracy : 0.7805\n",
            "Validation loss : 0.46475577\n",
            "Validation accuracy : 0.762\n",
            "At iteration 9500\n",
            "Training loss : 0.44622782\n",
            "Training accuracy : 0.7811\n",
            "Validation loss : 0.4647322\n",
            "Validation accuracy : 0.764\n",
            "At iteration 9600\n",
            "Training loss : 0.44618705\n",
            "Training accuracy : 0.7815\n",
            "Validation loss : 0.46471325\n",
            "Validation accuracy : 0.764\n",
            "At iteration 9700\n",
            "Training loss : 0.446156\n",
            "Training accuracy : 0.7815\n",
            "Validation loss : 0.46469206\n",
            "Validation accuracy : 0.764\n",
            "At iteration 9800\n",
            "Training loss : 0.44612783\n",
            "Training accuracy : 0.7811\n",
            "Validation loss : 0.4646779\n",
            "Validation accuracy : 0.766\n",
            "At iteration 9900\n",
            "Training loss : 0.44610536\n",
            "Training accuracy : 0.7813\n",
            "Validation loss : 0.464679\n",
            "Validation accuracy : 0.766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Octmbzrf--v6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "01abcf34-a7c7-4d8a-d29d-1ed6587ab209"
      },
      "cell_type": "code",
      "source": [
        "print_model(model3,testx)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdcc15fbb00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "UYXYoGl2_A-4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}